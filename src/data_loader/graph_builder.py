# Ez a szkript felel≈ës a dokumentumok metaadataib√≥l egy h√°l√≥zati gr√°f fel√©p√≠t√©s√©√©rt,
# amely a dokumentumok, jogszab√°lyok √©s b√≠r√≥s√°gok k√∂z√∂tti kapcsolatokat reprezent√°lja.
import sys
import pandas as pd
import networkx as nx
import json
import io
import pickle
from tqdm import tqdm
import logging
from collections import Counter, defaultdict
from datetime import datetime, timezone
from pathlib import Path

# Loggol√°s alapbe√°ll√≠t√°sa
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Konfigur√°ci√≥s be√°ll√≠t√°sok √©s seg√©dprogramok import√°l√°sa
try:
    from configs import config
    from src.utils.azure_blob_storage import AzureBlobStorage
except ImportError as e:
    print(f"HIBA: Modul import√°l√°sa sikertelen: {e}")
    sys.exit(1)

# --- Seg√©df√ºggv√©nyek ---

def parse_list_string(data_string, separator=';'):
    """Egy stringk√©nt t√°rolt, elv√°laszt√≥val tagolt listaelemet alak√≠t √°t val√≥di list√°v√°."""
    if not data_string or pd.isna(data_string):
        return []
    
    # Handle JSON list format
    if isinstance(data_string, str) and data_string.strip().startswith('[') and data_string.strip().endswith(']'):
        try:
            parsed_list = json.loads(data_string)
            if isinstance(parsed_list, list):
                return [str(item).strip() for item in parsed_list if item]
        except json.JSONDecodeError:
            pass
    
    # Handle regular string with separator
    if isinstance(data_string, str):
        return [item.strip() for item in data_string.split(separator) if item.strip()]
    
    # Handle direct list input
    if isinstance(data_string, list):
        return [str(item).strip() for item in data_string if item]
    
    return []

def is_valid_doc_id(doc_id):
    """Alapvet≈ë ellen≈ërz√©s a dokumentumazonos√≠t√≥kra (legyen string √©s ne legyen √ºres)."""
    return isinstance(doc_id, str) and bool(doc_id.strip())

# --- F≈ë gr√°f√©p√≠t≈ë logika ---

def build_graph(df: pd.DataFrame, stop_jogszabalyok: set) -> nx.DiGraph:
    """Fel√©p√≠ti a NetworkX gr√°fot a bemeneti DataFrame alapj√°n - optimaliz√°lt verzi√≥."""
    G = nx.DiGraph()
    logging.info("Gr√°f√©p√≠t√©s megkezd√©se...")

    # Batch m≈±veletek el≈ëk√©sz√≠t√©se
    batch_nodes = []
    batch_edges = []
    edge_weights = defaultdict(int)
    
    for _, doc_data in tqdm(df.iterrows(), total=df.shape[0], desc="Gr√°f √©p√≠t√©se"): # tqdm progress bar
        doc_id = doc_data.get('doc_id')
        if not is_valid_doc_id(doc_id):
            logging.debug(f"√ârv√©nytelen vagy hi√°nyz√≥ doc_id ({doc_id}), a sor kihagyva.")
            continue

        # Adatmez≈ëk kinyer√©se √©s list√°v√° alak√≠t√°sa
        jogszabalyhelyek = parse_list_string(doc_data.get('Jogszabalyhelyek', ''))
        kapcsolodo_hatarozatok = parse_list_string(doc_data.get('KapcsolodoHatarozatok', ''))
        kapcsolodo_birosagok = parse_list_string(doc_data.get('AllKapcsolodoBirosag', ''))
        
        # Dokumentum csom√≥pont batch-hez
        node_attrs = {
            "type": "dokumentum",
            "jogterulet": doc_data.get('jogterulet') if pd.notna(doc_data.get('jogterulet')) else None,
            "birosag": doc_data.get('birosag') if pd.notna(doc_data.get('birosag')) else None,
            "ev": int(doc_data.get('HatarozatEve')) if pd.notna(doc_data.get('HatarozatEve')) and str(doc_data.get('HatarozatEve')).isdigit() else None,
        }
        node_attrs = {k: v for k, v in node_attrs.items() if v is not None}
        batch_nodes.append((doc_id, node_attrs))

        # Hivatkoz√°sok batch-hez
        for hatarozat_id in kapcsolodo_hatarozatok:
            if is_valid_doc_id(hatarozat_id):
                batch_nodes.append((hatarozat_id, {"type": "dokumentum"}))
                edge_key = (doc_id, hatarozat_id, "hivatkozik")
                edge_weights[edge_key] += 1

        # B√≠r√≥s√°gi kapcsolatok batch-hez
        for birosag_name in kapcsolodo_birosagok:
            if birosag_name and isinstance(birosag_name, str):
                birosag_node_id = f"birosag_{birosag_name.lower().replace(' ', '_')}"
                batch_nodes.append((birosag_node_id, {"type": "birosag", "name": birosag_name}))
                edge_key = (doc_id, birosag_node_id, "targyalta")
                edge_weights[edge_key] += 1

        # Jogszab√°lyhelyek batch-hez
        for jsz in jogszabalyhelyek:
            if jsz and isinstance(jsz, str) and jsz not in stop_jogszabalyok:
                jsz_node_id = f"jogszabaly_{jsz.lower().replace(' ', '_').replace('.', '').replace('¬ß', 'par').replace('(', '').replace(')', '')}"
                batch_nodes.append((jsz_node_id, {"type": "jogszabaly", "reference": jsz}))
                edge_key = (doc_id, jsz_node_id, "hivatkozik_jogszabalyra")
                edge_weights[edge_key] += 1

    # Batch csom√≥pont hozz√°ad√°s - duplik√°tumok kezel√©se
    logging.info("Csom√≥pontok batch hozz√°ad√°sa...")
    unique_nodes = {}
    for node_id, attrs in batch_nodes:
        if node_id in unique_nodes:
            # Attrib√∫tumok egyes√≠t√©se
            unique_nodes[node_id].update({k: v for k, v in attrs.items() if v is not None})
        else:
            unique_nodes[node_id] = attrs
    
    G.add_nodes_from(unique_nodes.items())

    # Batch √©l hozz√°ad√°s s√∫lyokkal
    logging.info("√âlek batch hozz√°ad√°sa...")
    edges_with_attrs = []
    for (u, v, rel_type), weight in edge_weights.items():
        edges_with_attrs.append((u, v, {"relation_type": rel_type, "weight": weight}))
    
    G.add_edges_from(edges_with_attrs)

    # Gr√°f metaadatok hozz√°ad√°sa
    G.graph['creation_timestamp_utc'] = datetime.now(timezone.utc).isoformat()
    G.graph['document_count'] = sum(1 for _, attrs in G.nodes(data=True) if attrs.get('type') == 'dokumentum')
    G.graph['stop_jogszabalyok_count'] = len(stop_jogszabalyok)

    logging.info(f"Gr√°f√©p√≠t√©s befejezve. Csom√≥pontok: {G.number_of_nodes()}, √âlek: {G.number_of_edges()}")
    return G

def determine_stop_jogszabalyok(df: pd.DataFrame, column_name='Jogszabalyhelyek', threshold_percentage=0.01) -> set:
    """Meghat√°rozza a gyakori jogszab√°lyhelyeket, amelyek "stop szavakk√©nt" funkcion√°lnak."""
    logging.info(f"Stop jogszab√°lyok meghat√°roz√°sa {threshold_percentage*100:.2f}% k√ºsz√∂b√©rt√©kkel...")
    
    if column_name not in df.columns:
        logging.warning(f"Column '{column_name}' not found in DataFrame. Cannot determine stop words.")
        return set()
    
    all_references = []
    for references_str in df[column_name].dropna():
        all_references.extend(parse_list_string(references_str))
    
    if not all_references:
        logging.warning("Nincsenek jogszab√°lyi hivatkoz√°sok az elemz√©shez.")
        return set()
    
    reference_counts = Counter(all_references)
    threshold_count = len(df) * threshold_percentage
    stop_set = {ref for ref, count in reference_counts.items() if count > threshold_count}
    
    logging.info(f"Tal√°lt {len(stop_set)} stop jogszab√°ly, amelyek az iratok t√∂bb mint {threshold_percentage*100:.2f}%-√°ban el≈ëfordulnak.")
    return stop_set

def main():
    """F≈ë f√ºggv√©ny az adatok bet√∂lt√©s√©hez, a gr√°f fel√©p√≠t√©s√©hez √©s a kimenetek ment√©s√©hez Azure integr√°ci√≥val."""
    logging.info("üöÄ GR√ÅF√âP√çT≈ê IND√çT√ÅSA AZURE BLOB STORAGE ALAPJ√ÅN")

    # Azure Blob Storage kliens
    try:
        blob_storage = AzureBlobStorage(container_name=config.AZURE_CONTAINER_NAME)
    except ValueError as e:
        logging.error(e)
        sys.exit(1)

    # 1. Adatok let√∂lt√©se
    input_blob = config.BLOB_CLEANED_DOCUMENTS_PARQUET
    logging.info(f"Adatok let√∂lt√©se: {input_blob}")
    try:
        data = blob_storage.download_data(input_blob)
        df = pd.read_parquet(io.BytesIO(data))
        logging.info(f"‚úÖ Adatok bet√∂ltve: {len(df):,} dokumentum.")
    except Exception as e:
        logging.error(f"Hiba a bemeneti adatok let√∂lt√©sekor: {e}", exc_info=True)
        sys.exit(1)
    
    # 2. Stop szavak meghat√°roz√°sa
    stop_jogszabalyok = determine_stop_jogszabalyok(df)

    # 3. Gr√°f √©p√≠t√©se
    G = build_graph(df, stop_jogszabalyok)

    # 4. Gr√°f ment√©se √©s felt√∂lt√©se
    output_blob = config.BLOB_GRAPH
    logging.info(f"Gr√°f ment√©se √©s felt√∂lt√©se ide: {output_blob}")
    try:
        buffer = io.BytesIO()
        pickle.dump(G, buffer)
        buffer.seek(0)
        blob_storage.upload_data(buffer.getvalue(), output_blob)
        logging.info("‚úÖ Gr√°f sikeresen felt√∂ltve.")
    except Exception as e:
        logging.error(f"Hiba a gr√°f ment√©se vagy felt√∂lt√©se k√∂zben: {e}", exc_info=True)
        sys.exit(1)

    logging.info("\nüéâ GR√ÅF√âP√çT√âS BEFEJEZVE!")

if __name__ == "__main__":
    main()