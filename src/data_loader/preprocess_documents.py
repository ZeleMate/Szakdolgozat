# Ez a szkript felel≈ës a nyers dokumentumok (RTF, DOCX) √©s a hozz√°juk tartoz√≥
# JSON metaadatok feldolgoz√°s√°√©rt. A szkript el≈ësz√∂r let√∂lti a nyers adatokat
# egy ideiglenes helyi k√∂nyvt√°rba, ott feldolgozza ≈ëket, majd az eredm√©ny√ºl
# kapott egys√©ges Parquet f√°jlt felt√∂lti az Azure Blob Storage-ba.
import pandas as pd
import json
import re
from pathlib import Path
from tqdm import tqdm
import sys
import os
import logging
from striprtf.striprtf import rtf_to_text
import csv
from docx import Document
from bs4 import BeautifulSoup
import html
import io
import tempfile
import shutil

# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Konfigur√°ci√≥ √©s seg√©dprogramok import√°l√°sa
try:
    from configs import config
    from src.utils.azure_blob_storage import AzureBlobStorage
except ImportError as e:
    print(f"HIBA: Modul import√°l√°sa sikertelen: {e}")
    sys.exit(1)

# Loggol√°s be√°ll√≠t√°sa
logging.basicConfig(level=config.LOGGING_LEVEL, format=config.LOGGING_FORMAT)

# Az Azure SDK t√∫lzottan b≈ëbesz√©d≈± napl√≥z√°s√°nak korl√°toz√°sa WARNING szintre.
logging.getLogger("azure").setLevel(logging.WARNING)

# Azure Blob Storage kliens inicializ√°l√°sa
try:
    blob_storage = AzureBlobStorage(container_name=config.AZURE_CONTAINER_NAME)
except ValueError as e:
    logging.error(e)
    sys.exit(1)


def clean_text_for_embedding(text: str) -> str:
    """
    Sz√∂veg tiszt√≠t√°sa embedding gener√°l√°s el≈ëtt.
    Elt√°vol√≠tja a HTML tageket, URL-eket √©s egy√©b technikai zajt.
    """
    if not isinstance(text, str) or not text.strip():
        return ""
    
    # HTML entit√°sok dek√≥dol√°sa (pl. &amp; -> &)
    try:
        text = html.unescape(text)
    except Exception:
        pass # Ha hiba t√∂rt√©nik, a nyers sz√∂veggel megy√ºnk tov√°bb

    # HTML tagek elt√°vol√≠t√°sa BeautifulSoup-pal
    try:
        soup = BeautifulSoup(text, 'html.parser')
        text = soup.get_text(separator=' ')
    except Exception:
        # Ha a BeautifulSoup hib√°t dob, egyszer≈± regex-szel pr√≥b√°ljuk
        text = re.sub(r'<[^>]+>', '', text)
    
    # URL-ek, email c√≠mek elt√°vol√≠t√°sa
    text = re.sub(r'http\S+|www\S+|https\S+|\S+@\S+', '', text, flags=re.MULTILINE)

    # Null byte √©s egy√©b nem sz√∂veges vez√©rl≈ë karakterek elt√°vol√≠t√°sa
    text = text.replace('\x00', '')
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    # RTF specifikus maradv√°nyok elt√°vol√≠t√°sa, amik a konverzi√≥ ut√°n maradhatnak
    text = re.sub(r'\\[a-zA-Z]+\d*\s?', '', text)
    text = re.sub(r'[{}]', '', text)
    
    # T√∂bbsz√∂r√∂s sz√≥k√∂z√∂k, tabul√°torok, √∫j sorok cser√©je egyetlen sz√≥k√∂zre
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def clean_surrogates(text):
    """Elt√°vol√≠tja az √©rv√©nytelen surrogate p√°rokat a stringb≈ël."""
    if not isinstance(text, str):
        return text
    return re.sub(r'[\ud800-\udfff]', '', text)

# T√°mogatott sz√∂vegf√°jl kiterjeszt√©sek
SUPPORTED_EXTENSIONS = tuple(ext.lower() for ext in config.SUPPORTED_TEXT_EXTENSIONS)

def main():
    """
    Let√∂lti a nyers adatokat egy ideiglenes helyi k√∂nyvt√°rba,
    lok√°lisan feldolgozza ≈ëket, majd a tiszta Parquet f√°jlt felt√∂lti az Azure-ba.
    """
    project_root = Path(__file__).resolve().parent.parent.parent

    # Perzisztens helyi gyors√≠t√≥t√°r a nyers adatoknak, hogy ne kelljen √∫jra let√∂lteni
    local_raw_data_dir = project_root / 'data_cache' / 'raw_documents'
    local_raw_data_dir.mkdir(parents=True, exist_ok=True)
    logging.info(f"Helyi gyors√≠t√≥t√°r haszn√°lata a nyers adatokhoz: {local_raw_data_dir}")

    # A kimeneti Parquet f√°jlnak egy val√≥ban ideiglenes k√∂nyvt√°rat haszn√°lunk, ami t√∂rl≈ëdni fog
    output_processing_dir = tempfile.mkdtemp()
    logging.info(f"Ideiglenes kimeneti k√∂nyvt√°r l√©trehozva: {output_processing_dir}")

    total_records = 0
    success = False
    
    try:
        # 1. ===== ADATOK LET√ñLT√âSE AZURE BLOB-B√ìL LOK√ÅLIS GYORS√çT√ìT√ÅRBA =====
        logging.info("Nyers adatok szinkroniz√°l√°sa a helyi gyors√≠t√≥t√°rral...")
        all_blob_paths = blob_storage.list_blobs(path_prefix=config.BLOB_RAW_DATA_DIR)
        
        if not all_blob_paths:
            logging.warning(f"Nem tal√°lhat√≥ak blobok a '{config.BLOB_RAW_DATA_DIR}/' prefix alatt. A szkript le√°ll.")
            return

        for blob_path in tqdm(all_blob_paths, desc="F√°jlok szinkroniz√°l√°sa a helyi gyors√≠t√≥t√°rba"):
            try:
                relative_blob_path = Path(*Path(blob_path).parts[1:])
                local_path = local_raw_data_dir / relative_blob_path

                # Csak akkor t√∂ltj√ºk le, ha a f√°jl m√©g nem l√©tezik lok√°lisan
                if local_path.exists():
                    continue

                local_path.parent.mkdir(parents=True, exist_ok=True)
                
                data = blob_storage.download_data(blob_path)
                with open(local_path, "wb") as f:
                    f.write(data)
            except Exception as e:
                logging.error(f"Hiba a(z) {blob_path} blob let√∂lt√©sekor: {e}", exc_info=True)
        
        logging.info(f"Helyi gyors√≠t√≥t√°r szinkroniz√°lva. √ñsszesen {len(all_blob_paths):,} f√°jl ellen≈ërizve.")

        # 2. ===== LOK√ÅLIS F√ÅJLOK FELDOLGOZ√ÅSA A GYORS√çT√ìT√ÅRB√ìL =====
        logging.info("Helyi f√°jlok feldolgoz√°s√°nak megkezd√©se a gyors√≠t√≥t√°rb√≥l...")
        
        document_files = {}
        json_files = {}

        all_local_files = [p for p in local_raw_data_dir.rglob('*') if p.is_file()]

        for file_path in all_local_files:
            if file_path.suffix.lower() in SUPPORTED_EXTENSIONS:
                base_name = file_path.stem
                document_files[base_name] = file_path
            elif file_path.suffix.lower() == '.json':
                base_name = re.sub(r'\.(RTF|DOCX)_OBH$', '', file_path.stem, flags=re.IGNORECASE)
                json_files[base_name] = file_path

        all_records = []
        logging.info(f"Feldolgoz√°sra v√°r√≥ dokumentum p√°rok sz√°ma: {len(document_files):,}")

        for base_filename, text_filepath in tqdm(document_files.items(), desc="Dokumentumok feldolgoz√°sa"):
            json_filepath = json_files.get(base_filename)

            text_content = ""
            try:
                if text_filepath.suffix.lower() == '.rtf':
                    with open(text_filepath, 'r', encoding='utf-8', errors='ignore') as f:
                        rtf_content = f.read()
                    text_content = rtf_to_text(rtf_content, errors="ignore")
                elif text_filepath.suffix.lower() == '.docx':
                    doc = Document(text_filepath)
                    text_content = ' \\n'.join(para.text for para in doc.paragraphs if para.text.strip())
            except Exception as e:
                logging.warning(f"Nem siker√ºlt kinyerni a sz√∂veget a f√°jlb√≥l ({text_filepath}): {e}")
                continue
            
            cleaned_text_content = clean_text_for_embedding(text_content)

            if len(cleaned_text_content) < config.CLEANING_MIN_TEXT_LENGTH:
                logging.debug(f"Dokumentum √°tugorva, mert a tiszt√≠tott sz√∂veg t√∫l r√∂vid: {text_filepath.name}")
                continue

            extracted_metadata = {}
            all_related_ugyszam = []
            all_related_birosag = []

            if json_filepath:
                try:
                    with open(json_filepath, 'r', encoding='utf-8', errors='ignore') as f:
                        metadata_dict = json.load(f)
                    
                    if 'List' in metadata_dict and isinstance(metadata_dict['List'], list) and len(metadata_dict['List']) > 0:
                        extracted_metadata = metadata_dict['List'][0]
                        if 'KapcsolodoHatarozatok' in extracted_metadata and isinstance(extracted_metadata['KapcsolodoHatarozatok'], list):
                            for related_case in extracted_metadata['KapcsolodoHatarozatok']:
                                if isinstance(related_case, dict):
                                    all_related_ugyszam.append(related_case.get('KapcsolodoUgyszam'))
                                    all_related_birosag.append(related_case.get('KapcsolodoBirosag'))
                                else:
                                    logging.warning(f"A KapcsolodoHatarozatok lista egyik eleme nem sz√≥t√°r a {json_filepath} f√°jlban.")
                                    all_related_ugyszam.append(None)
                                    all_related_birosag.append(None)
                        if 'Jogszabalyhelyek' in extracted_metadata and not isinstance(extracted_metadata['Jogszabalyhelyek'], (str, int, float, bool)):
                            extracted_metadata['Jogszabalyhelyek'] = json.dumps(extracted_metadata['Jogszabalyhelyek'], ensure_ascii=False)
                        if 'KapcsolodoHatarozatok' in extracted_metadata and not isinstance(extracted_metadata['KapcsolodoHatarozatok'], (str, int, float, bool)):
                            extracted_metadata['KapcsolodoHatarozatok'] = json.dumps(extracted_metadata['KapcsolodoHatarozatok'], ensure_ascii=False)
                except json.JSONDecodeError:
                    logging.warning(f"Nem siker√ºlt dek√≥dolni a JSON f√°jlt: {json_filepath}")
                except Exception as e:
                    logging.warning(f"Hiba a JSON f√°jl feldolgoz√°sa k√∂zben ({json_filepath}): {e}")

            birosag_from_path = None
            try:
                relative_path_parts = text_filepath.relative_to(local_raw_data_dir).parts
                if len(relative_path_parts) > 1:
                     birosag_from_path = relative_path_parts[0]
            except Exception as e_path:
                 logging.warning(f"V√°ratlan hiba a b√≠r√≥s√°g nev√©nek √∫tvonalb√≥l t√∂rt√©n≈ë kinyer√©se k√∂zben ({text_filepath}): {e_path}")

            record = {
                'text': cleaned_text_content,
                **extracted_metadata,
                'AllKapcsolodoUgyszam': json.dumps(all_related_ugyszam, ensure_ascii=False) if all_related_ugyszam else None,
                'AllKapcsolodoBirosag': json.dumps(all_related_birosag, ensure_ascii=False) if all_related_birosag else None,
            }
            record['doc_id'] = extracted_metadata.get('Azonosito', base_filename)
            record['birosag'] = extracted_metadata.get('MeghozoBirosag', birosag_from_path)
            
            record.pop('Szoveg', None)
            record.pop('RezumeSzovegKornyezet', None)
            record.pop('DownloadLink', None)
            record.pop('metadata', None)

            all_records.append(record)
        
        total_records = len(all_records)

        # 3. ===== EGYES√çTETT, TISZT√çTOTT PARQUET L√âTREHOZ√ÅSA √âS FELT√ñLT√âSE =====
        if not all_records:
            logging.warning("Nincs feldolgozhat√≥ rekord, a Parquet f√°jl l√©trehoz√°sa √©s felt√∂lt√©se √°tugorva.")
            return

        logging.info("Feldolgoz√°s befejezve, egys√©ges DataFrame l√©trehoz√°sa √©s felt√∂lt√©se...")
        
        df = pd.DataFrame(all_records)
        for col in df.select_dtypes(include=['object']).columns:
            df[col] = df[col].apply(clean_surrogates)

        df['birosag'] = df['birosag'].fillna('ISMERETLEN')

        expected_cols = [
            'doc_id', 'text', 'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',
            'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', 'AllKapcsolodoBirosag',
            'KapcsolodoHatarozatok', 'Jogszabalyhelyek'
        ]
        
        final_cols = [col for col in expected_cols if col in df.columns]
        other_cols = [col for col in df.columns if col not in final_cols]
        df = df[final_cols + other_cols]

        local_parquet_path = Path(output_processing_dir) / "cleaned_documents.parquet"
        df.to_parquet(
            path=local_parquet_path,
            engine='pyarrow',
            compression='snappy',
            index=False,
        )
        
        logging.info(f"Tiszt√≠tott Parquet f√°jl sikeresen felt√∂ltve ide: {local_parquet_path}")

        # 4. ===== PARQUET F√ÅJL FELT√ñLT√âSE AZURE BLOB-BA =====
        logging.info(f"Parquet f√°jl felt√∂lt√©se a(z) '{config.AZURE_CONTAINER_NAME}' kont√©nerbe...")
        blob_path = config.BLOB_CLEANED_DOCUMENTS_PARQUET
        
        with open(local_parquet_path, "rb") as data:
            blob_storage.upload_data(data=data.read(), blob_path=blob_path)
        
        logging.info(f"Tiszt√≠tott Parquet f√°jl sikeresen felt√∂ltve ide: {blob_path} ({len(df):,} sor)")
        success = True

    except Exception as e:
        logging.error(f"Hiba t√∂rt√©nt a f≈ë feldolgoz√°si folyamatban: {e}", exc_info=True)
    finally:
        # 5. ===== IDEIGLENES KIMENETI K√ñNYVT√ÅR T√ñRL√âSE =====
        logging.info(f"√Åtmeneti kimeneti k√∂nyvt√°r t√∂rl√©se: {output_processing_dir}")
        shutil.rmtree(output_processing_dir, ignore_errors=True)
        # A nyers adatok gyors√≠t√≥t√°ra (`local_raw_data_dir`) megmarad a k√∂vetkez≈ë futtat√°shoz.

        if success:
            print(f"\n‚úÖ PREPROCESSING BEFEJEZVE!")
            print(f"üìä Feldolgozott rekordok: {total_records:,}")
            print(f"üìÑ Kimeneti blob: {config.AZURE_CONTAINER_NAME}/{config.BLOB_CLEANED_DOCUMENTS_PARQUET}")
        else:
            print(f"\n‚ùå PREPROCESSING SIKERTELEN!")
            print("K√©rj√ºk, ellen≈ërizze a logokat a hiba r√©szletei√©rt.")


if __name__ == "__main__":
    main()