# Ez a szkript felel≈ës a nyers dokumentumok (RTF, DOCX) √©s a hozz√°juk tartoz√≥
# JSON metaadatok feldolgoz√°s√°√©rt, majd chunked CSV f√°jlokba t√∂rt√©n≈ë ment√©s√©√©rt.
# M√ìDOS√çTVA: Memory-safe chunked ment√©s az OOM probl√©m√°k elker√ºl√©s√©re.
import pandas as pd
import json
import re
from pathlib import Path
from tqdm import tqdm
import sys
import os
import logging # logging import√°l√°sa
from striprtf.striprtf import rtf_to_text

# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = Path(__file__).resolve().parent.parent.parent  # data_loader -> src -> project_root
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Debug: config import ellen≈ërz√©se
configs_path = project_root / "configs"
if not configs_path.exists():
    print(f"HIBA: configs mappa nem tal√°lhat√≥: {configs_path}")
    print(f"Project root: {project_root}")
    print(f"Working directory: {os.getcwd()}")
    sys.exit(1)

try:
    from configs import config
except ImportError as e:
    print(f"HIBA: configs modul import sikertelen: {e}")
    print(f"Python path: {sys.path}")
    print(f"Configs path: {configs_path}")
    sys.exit(1)

# ===== CHUNKED MENT√âS KONFIGUR√ÅCI√ìJA =====
CHUNK_SIZE = 2000  # Rekordok sz√°ma chunk-onk√©nt (mem√≥ria optimaliz√°l√°shoz)
ENABLE_UNIFIED_CSV = True  # Egyes√≠tett CSV l√©trehoz√°sa backwards compatibility-√©rt

# Loggol√°s be√°ll√≠t√°sa a k√∂zponti konfigur√°ci√≥b√≥l
# Ennek a config import√°l√°sa UT√ÅN kell k√∂vetkeznie
logging.basicConfig(
    level=config.LOGGING_LEVEL,
    format=config.LOGGING_FORMAT,
    # force=True # Sz√ºks√©ges lehet, ha a root logger m√°r konfigur√°lva van m√°shol (pl. notebookban)
               # vagy ha a szkriptet t√∂bbsz√∂r import√°ljuk/futtatjuk ugyanabban a sessionben.
               # √ìvatosan haszn√°land√≥, mivel fel√ºl√≠rja a megl√©v≈ë be√°ll√≠t√°sokat.
)

def save_chunk_to_csv(chunk_records, chunk_idx, expected_cols):
    """
    Chunk ment√©se CSV f√°jlba a szemantikai kereshet≈ës√©g meg≈ërz√©s√©vel.
    """
    if not chunk_records:
        return None
    
    # DataFrame l√©trehoz√°sa
    df_chunk = pd.DataFrame(chunk_records)
    
    # Hi√°nyz√≥ oszlopok hozz√°ad√°sa (az eredeti logika alapj√°n)
    for col in expected_cols:
        if col not in df_chunk.columns:
            df_chunk[col] = None
    
    # Oszlopok sorrendj√©nek be√°ll√≠t√°sa (az eredeti logika alapj√°n)
    final_ordered_cols = [col for col in expected_cols if col in df_chunk.columns]
    other_cols = [col for col in df_chunk.columns if col not in final_ordered_cols]
    df_chunk = df_chunk[final_ordered_cols + other_cols]
    
    # Chunk f√°jl ment√©se
    chunk_filename = f"raw_chunk_{chunk_idx:04d}.csv"
    chunk_dir = config.OUT_DIR / "chunked_raw"
    chunk_dir.mkdir(parents=True, exist_ok=True)
    chunk_path = chunk_dir / chunk_filename
    
    df_chunk.to_csv(chunk_path, index=False, encoding=config.CSV_ENCODING, errors='replace')
    
    logging.info(f"Raw chunk mentve: {chunk_filename} ({len(df_chunk):,} rekord)")
    return chunk_path

# Adat k√∂nyvt√°r el√©r√©si √∫tja a konfigur√°ci√≥b√≥l
root_dir_to_scan = project_root / 'data' # Ez a k√∂nyvt√°r lesz rekurz√≠van bej√°rva
paths = list(root_dir_to_scan.rglob('*')) # Az √∂sszes f√°jl √©s mappa lek√©r√©se

# ===== CHUNKED FELDOLGOZ√ÅS V√ÅLTOZ√ìK =====
chunk_records = []  # Aktu√°lis chunk rekordjai (korl√°tozott m√©ret!)
chunk_idx = 0       # Chunk sorsz√°ma
saved_chunks = []   # Mentett chunk f√°jlok list√°ja
total_records = 0   # Statisztika

# T√°mogatott sz√∂vegf√°jl kiterjeszt√©sek
SUPPORTED_EXTENSIONS = tuple(ext.lower() for ext in config.SUPPORTED_TEXT_EXTENSIONS)

# V√°rt oszlopok (az eredeti logika alapj√°n) 
expected_cols_for_raw_csv = [
    'doc_id', 'text', 'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',
    'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', 'AllKapcsolodoBirosag',
    'KapcsolodoHatarozatok', 'Jogszabalyhelyek'
]

logging.info(f"Chunked feldolgoz√°s kezd√©se (chunk m√©ret: {CHUNK_SIZE:,})")
logging.info(f"Tal√°lva {len(paths):,} potenci√°lis f√°jl")

for path in tqdm(paths, desc="Dokumentumf√°jlok feldolgoz√°sa"): # tqdm progress bar
    if path.is_file() and path.suffix.lower() in SUPPORTED_EXTENSIONS:
        base_filename = path.stem # F√°jln√©v kiterjeszt√©s n√©lk√ºl
        text_path = path

        # A kapcsol√≥d√≥ JSON metaadat f√°jl nev√©nek k√©pz√©se
        # Pl. '123.docx' -> '123.RTF_OBH.JSON' (a logika alapj√°n az RTF_OBH fix)
        json_filename = base_filename + '.RTF_OBH.JSON'
        json_path = path.with_name(json_filename)

        text_content = "" # Alap√©rtelmezett √ºres sz√∂veg
        if text_path.suffix.lower() == '.rtf':
            try:
                with open(text_path, 'r', encoding='utf-8', errors='ignore') as f:
                    rtf_content = f.read()
                text_content = rtf_to_text(rtf_content, errors="ignore")
            except Exception as e:
                # Itt jobb lenne logging.warning vagy error
                print(f"Figyelmeztet√©s: Nem siker√ºlt kinyerni a sz√∂veget az RTF f√°jlb√≥l ({text_path}) a striprtf seg√≠ts√©g√©vel: {e}")
        elif text_path.suffix.lower() == '.docx':
            try:
                from docx import Document # Import√°l√°s csak itt, ha t√©nyleg sz√ºks√©g van r√°
                doc = Document(str(text_path))
                text_content = ' \n'.join(para.text for para in doc.paragraphs if para.text.strip()) # √úres paragrafusok kihagy√°sa
            except Exception as e:
                print(f"Figyelmeztet√©s: Nem siker√ºlt kinyerni a sz√∂veget a DOCX f√°jlb√≥l ({text_path}): {e}")
        
        # Sz√∂veg normaliz√°l√°sa: t√∂bbsz√∂r√∂s whitespace cser√©je egy sz√≥k√∂zre, felesleges sz√≥k√∂z√∂k elt√°vol√≠t√°sa az elej√©r≈ël/v√©g√©r≈ël
        text_content = re.sub(r'\s+', ' ', text_content).strip()

        extracted_metadata = {} # Kinyert metaadatok t√°rol√°s√°ra
        all_related_ugyszam = [] # Kapcsol√≥d√≥ √ºgysz√°mok list√°ja
        all_related_birosag = [] # Kapcsol√≥d√≥ b√≠r√≥s√°gok list√°ja

        if json_path.exists():
            try:
                with open(json_path, 'r', encoding='utf-8') as jf:
                    metadata_dict = json.load(jf)
                    # Felt√©telezz√ºk, hogy a relev√°ns adatok a 'List' kulcs alatt l√©v≈ë lista els≈ë elem√©ben vannak
                    if 'List' in metadata_dict and isinstance(metadata_dict['List'], list) and len(metadata_dict['List']) > 0:
                        extracted_metadata = metadata_dict['List'][0]
                        # Kapcsol√≥d√≥ hat√°rozatok adatainak kinyer√©se
                        if 'KapcsolodoHatarozatok' in extracted_metadata and isinstance(extracted_metadata['KapcsolodoHatarozatok'], list):
                            for related_case in extracted_metadata['KapcsolodoHatarozatok']:
                                if isinstance(related_case, dict):
                                    all_related_ugyszam.append(related_case.get('KapcsolodoUgyszam'))
                                    all_related_birosag.append(related_case.get('KapcsolodoBirosag'))
                                else:
                                    print(f"Figyelmeztet√©s: A KapcsolodoHatarozatok lista egyik eleme nem sz√≥t√°r a {json_path} f√°jlban.")
                                    all_related_ugyszam.append(None)
                                    all_related_birosag.append(None)
                        # √ñsszetett 'Jogszabalyhelyek' √©s 'KapcsolodoHatarozatok' stringg√© alak√≠t√°sa a CSV kompatibilit√°s √©rdek√©ben
                        if 'Jogszabalyhelyek' in extracted_metadata and not isinstance(extracted_metadata['Jogszabalyhelyek'], (str, int, float, bool)):
                            extracted_metadata['Jogszabalyhelyek'] = json.dumps(extracted_metadata['Jogszabalyhelyek'], ensure_ascii=False)
                        if 'KapcsolodoHatarozatok' in extracted_metadata and not isinstance(extracted_metadata['KapcsolodoHatarozatok'], (str, int, float, bool)):
                            extracted_metadata['KapcsolodoHatarozatok'] = json.dumps(extracted_metadata['KapcsolodoHatarozatok'], ensure_ascii=False)
            except json.JSONDecodeError:
                print(f"Figyelmeztet√©s: Nem siker√ºlt dek√≥dolni a JSON f√°jlt: {json_path}")
            except Exception as e:
                print(f"Figyelmeztet√©s: Hiba a JSON f√°jl feldolgoz√°sa k√∂zben ({json_path}): {e}")
        # else: # Ha nincs JSON, a metaadatok √ºresek maradnak
            # Ide lehetne loggol√°st tenni, ha hi√°nyzik a JSON, de a jelenlegi k√≥d csendben tov√°bbmegy.

        # B√≠r√≥s√°g nev√©nek kinyer√©se az el√©r√©si √∫tb√≥l (fallback)
        birosag_from_path = None
        try:
            abs_root_dir = root_dir_to_scan.resolve()
            abs_path = path.resolve()
            if abs_path.is_relative_to(abs_root_dir):
                 rel_parts = abs_path.relative_to(abs_root_dir).parts
                 if len(rel_parts) > 1:
                    birosag_from_path = rel_parts[0]
            # else: # Ha nem relat√≠v, nem tudjuk meg√°llap√≠tani
                 # print(f"Figyelmeztet√©s: Az √∫tvonal ({path}) nem relat√≠v a gy√∂k√©rhez ({root_dir_to_scan}). A b√≠r√≥s√°g nem √°llap√≠that√≥ meg az √∫tvonalb√≥l.")
        except Exception as e_path:
             print(f"Figyelmeztet√©s: V√°ratlan hiba a b√≠r√≥s√°g nev√©nek √∫tvonalb√≥l t√∂rt√©n≈ë kinyer√©se k√∂zben ({path}): {e_path}")

        record = {
            'text': text_content,
            **extracted_metadata, # Kinyert metaadatok hozz√°ad√°sa
            'AllKapcsolodoUgyszam': json.dumps(all_related_ugyszam, ensure_ascii=False) if all_related_ugyszam else None,
            'AllKapcsolodoBirosag': json.dumps(all_related_birosag, ensure_ascii=False) if all_related_birosag else None,
        }
        # doc_id be√°ll√≠t√°sa: els≈ëdlegesen a JSON-b√≥l ('Azonosito'), m√°sodlagosan a f√°jln√©vb≈ël
        record['doc_id'] = extracted_metadata.get('Azonosito', base_filename)
        # B√≠r√≥s√°g be√°ll√≠t√°sa: els≈ëdlegesen a JSON-b√≥l ('MeghozoBirosag'), m√°sodlagosan az √∫tvonalb√≥l
        record['birosag'] = extracted_metadata.get('MeghozoBirosag', birosag_from_path)

        # Potenci√°lisan probl√©m√°s vagy felesleges mez≈ëk elt√°vol√≠t√°sa
        record.pop('Szoveg', None) # Ha a JSON tartalmazta a teljes sz√∂veget, itt elt√°vol√≠tjuk
        record.pop('RezumeSzovegKornyezet', None)
        record.pop('DownloadLink', None)
        record.pop('metadata', None) # Ha a **extracted_metadata hozz√°adta volna a teljes 'List' objektumot

        # ===== CHUNKED MENT√âS =====
        chunk_records.append(record)
        total_records += 1
        
        # Ha el√©rte a chunk m√©retet, ment√©s √©s reset
        if len(chunk_records) >= CHUNK_SIZE:
            chunk_path = save_chunk_to_csv(chunk_records, chunk_idx, expected_cols_for_raw_csv)
            if chunk_path:
                saved_chunks.append(chunk_path)
            chunk_records = []  # Reset a mem√≥ria felszabad√≠t√°s√°hoz
            chunk_idx += 1

# ===== UTOLS√ì CHUNK MENT√âSE =====
if chunk_records:
    chunk_path = save_chunk_to_csv(chunk_records, chunk_idx, expected_cols_for_raw_csv)
    if chunk_path:
        saved_chunks.append(chunk_path)

# ===== √ñSSZEGZ≈ê STATISZTIK√ÅK =====
logging.info(f"Chunked feldolgoz√°s befejezve:")
logging.info(f"  Feldolgozott rekordok: {total_records:,}")
logging.info(f"  Mentett chunk-ok: {len(saved_chunks)}")
logging.info(f"  Chunk-ok mapp√°ja: {config.OUT_DIR / 'chunked_raw'}")

# ===== OPCION√ÅLIS EGYES√çTETT CSV (BACKWARDS COMPATIBILITY) =====
unified_csv_created = False
if ENABLE_UNIFIED_CSV and saved_chunks:
    logging.info("Egyes√≠tett CSV l√©trehoz√°sa backwards compatibility-√©rt...")
    logging.warning("FIGYELEM: Ez megn√∂veli a mem√≥riahaszn√°latot!")
    
    try:
        # Chunk-ok egyes√≠t√©se
        all_chunk_dfs = []
        for chunk_path in saved_chunks:
            chunk_df = pd.read_csv(chunk_path, encoding=config.CSV_ENCODING)
            all_chunk_dfs.append(chunk_df)
        
        # Egyes√≠tett DataFrame
        unified_df = pd.concat(all_chunk_dfs, ignore_index=True)
        
        # Egyes√≠tett CSV ment√©se (az eredeti helyre)
        out_path = config.RAW_CSV_DATA_PATH
        out_path.parent.mkdir(parents=True, exist_ok=True)
        unified_df.to_csv(out_path, index=False, encoding=config.CSV_ENCODING, errors='replace')
        
        unified_csv_created = True
        logging.info(f"Egyes√≠tett CSV mentve: {out_path} ({len(unified_df):,} sor)")
        
        # Mem√≥ria felszabad√≠t√°s
        del all_chunk_dfs, unified_df
        
    except Exception as e:
        logging.error(f"Hiba az egyes√≠tett CSV l√©trehoz√°s√°ban: {e}")
        logging.info("A chunk-ok tov√°bbra is el√©rhet≈ëk a chunked_raw mapp√°ban.")

# ===== V√âGS≈ê √úZENETEK =====
print(f"\n‚úÖ CHUNKED PREPROCESSING BEFEJEZVE!")
print(f"üìä Feldolgozott rekordok: {total_records:,}")
print(f"üìÅ Chunk f√°jlok ({len(saved_chunks)} db): {config.OUT_DIR / 'chunked_raw'}")

if unified_csv_created:
    print(f"üìÑ Egyes√≠tett CSV (backwards compatibility): {config.RAW_CSV_DATA_PATH}")
    print(f"üí° K√∂vetkez≈ë scriptek haszn√°lhatj√°k az egyes√≠tett CSV-t vagy a chunk-okat")
else:
    print(f"‚ö†Ô∏è  Nincs egyes√≠tett CSV - csak chunk-ok (mem√≥ria k√≠m√©l√©s)")
    print(f"üí° K√∂vetkez≈ë l√©p√©s: eda_clean_for_embedding.py m√≥dos√≠t√°sa chunked olvas√°sra")

print(f"üöÄ Memory haszn√°lat optimaliz√°lva: max {CHUNK_SIZE:,} rekord mem√≥ri√°ban egyszerre")