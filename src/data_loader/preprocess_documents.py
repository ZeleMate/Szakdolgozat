# Ez a szkript felel≈ës a nyers dokumentumok (RTF, DOCX) √©s a hozz√°juk tartoz√≥
# JSON metaadatok feldolgoz√°s√°√©rt, majd egyetlen CSV f√°jlba t√∂rt√©n≈ë ment√©s√©√©rt.
import pandas as pd
import json
import re
from pathlib import Path
from tqdm import tqdm
import sys
import os
import logging
from striprtf.striprtf import rtf_to_text
import csv
from docx import Document
from bs4 import BeautifulSoup
import html

# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Konfigur√°ci√≥ import√°l√°sa
try:
    from configs import config
except ImportError as e:
    print(f"HIBA: configs modul import sikertelen: {e}")
    sys.exit(1)

# Loggol√°s be√°ll√≠t√°sa
logging.basicConfig(level=config.LOGGING_LEVEL, format=config.LOGGING_FORMAT)

def clean_text_for_embedding(text: str) -> str:
    """
    Sz√∂veg alapos tiszt√≠t√°sa embedding gener√°l√°s el≈ëtt.
    Elt√°vol√≠tja a HTML tageket, speci√°lis karaktereket, URL-eket, √©s normaliz√°lja a whitespace-t.
    """
    if not isinstance(text, str) or not text.strip():
        return ""
    
    # HTML entit√°sok dek√≥dol√°sa (pl. &amp; -> &)
    try:
        text = html.unescape(text)
    except Exception:
        pass # Ha hiba t√∂rt√©nik, a nyers sz√∂veggel megy√ºnk tov√°bb

    # HTML tagek elt√°vol√≠t√°sa BeautifulSoup-pal
    try:
        soup = BeautifulSoup(text, 'html.parser')
        text = soup.get_text(separator=' ')
    except Exception:
        # Ha a BeautifulSoup hib√°t dob, egyszer≈± regex-szel pr√≥b√°ljuk
        text = re.sub(r'<[^>]+>', '', text)
    
    # URL-ek, email c√≠mek elt√°vol√≠t√°sa
    text = re.sub(r'http\S+|www\S+|https\S+|\S+@\S+', '', text, flags=re.MULTILINE)

    # Null byte √©s egy√©b nem sz√∂veges vez√©rl≈ë karakterek elt√°vol√≠t√°sa
    text = text.replace('\x00', '')
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    # RTF specifikus maradv√°nyok elt√°vol√≠t√°sa, amik a konverzi√≥ ut√°n maradhatnak
    text = re.sub(r'\\[a-zA-Z]+\d*\s?', '', text)
    text = re.sub(r'[{}]', '', text)
    
    # Egys√©ges kisbet≈±re alak√≠t√°s
    text = text.lower()
    
    # T√∂bbsz√∂r√∂s sz√≥k√∂z√∂k, tabul√°torok, √∫j sorok cser√©je egyetlen sz√≥k√∂zre
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# Adat k√∂nyvt√°r el√©r√©si √∫tja a konfigur√°ci√≥b√≥l
root_dir_to_scan = project_root / 'data'
paths = list(root_dir_to_scan.rglob('*'))

# A feldolgozott rekordokat egyetlen list√°ban gy≈±jtj√ºk
all_records = []
total_records = 0

# T√°mogatott sz√∂vegf√°jl kiterjeszt√©sek
SUPPORTED_EXTENSIONS = tuple(ext.lower() for ext in config.SUPPORTED_TEXT_EXTENSIONS)

logging.info(f"Feldolgoz√°s kezd√©se, c√©l: egyetlen CSV f√°jl.")
logging.info(f"Tal√°lva {len(paths):,} potenci√°lis f√°jl")

for path in tqdm(paths, desc="Dokumentumf√°jlok feldolgoz√°sa"):
    if path.is_file() and path.suffix.lower() in SUPPORTED_EXTENSIONS:
        base_filename = path.stem
        text_path = path
        json_filename = base_filename + '.RTF_OBH.JSON'
        json_path = path.with_name(json_filename)

        text_content = ""
        if text_path.suffix.lower() == '.rtf':
            try:
                with open(text_path, 'r', encoding='utf-8', errors='ignore') as f:
                    rtf_content = f.read()
                text_content = rtf_to_text(rtf_content, errors="ignore")
            except Exception as e:
                logging.warning(f"Nem siker√ºlt kinyerni a sz√∂veget az RTF f√°jlb√≥l ({text_path}): {e}")
        elif text_path.suffix.lower() == '.docx':
            try:
                doc = Document(str(text_path))
                text_content = ' \n'.join(para.text for para in doc.paragraphs if para.text.strip())
            except Exception as e:
                logging.warning(f"Nem siker√ºlt kinyerni a sz√∂veget a DOCX f√°jlb√≥l ({text_path}): {e}")
        
        # A kinyert nyers sz√∂veg azonnali tiszt√≠t√°sa
        cleaned_text_content = clean_text_for_embedding(text_content)

        # Csak akkor dolgozzuk fel a rekordot, ha a tiszt√≠t√°s ut√°n is maradt √©rt√©kelhet≈ë sz√∂veg
        if len(cleaned_text_content) < config.CLEANING_MIN_TEXT_LENGTH:
            logging.debug(f"Dokumentum √°tugorva, mert a tiszt√≠tott sz√∂veg t√∫l r√∂vid: {path.name}")
            continue

        extracted_metadata = {}
        all_related_ugyszam = []
        all_related_birosag = []

        if json_path.exists():
            try:
                with open(json_path, 'r', encoding='utf-8') as jf:
                    metadata_dict = json.load(jf)
                    if 'List' in metadata_dict and isinstance(metadata_dict['List'], list) and len(metadata_dict['List']) > 0:
                        extracted_metadata = metadata_dict['List'][0]
                        if 'KapcsolodoHatarozatok' in extracted_metadata and isinstance(extracted_metadata['KapcsolodoHatarozatok'], list):
                            for related_case in extracted_metadata['KapcsolodoHatarozatok']:
                                if isinstance(related_case, dict):
                                    all_related_ugyszam.append(related_case.get('KapcsolodoUgyszam'))
                                    all_related_birosag.append(related_case.get('KapcsolodoBirosag'))
                                else:
                                    logging.warning(f"A KapcsolodoHatarozatok lista egyik eleme nem sz√≥t√°r a {json_path} f√°jlban.")
                                    all_related_ugyszam.append(None)
                                    all_related_birosag.append(None)
                        if 'Jogszabalyhelyek' in extracted_metadata and not isinstance(extracted_metadata['Jogszabalyhelyek'], (str, int, float, bool)):
                            extracted_metadata['Jogszabalyhelyek'] = json.dumps(extracted_metadata['Jogszabalyhelyek'], ensure_ascii=False)
                        if 'KapcsolodoHatarozatok' in extracted_metadata and not isinstance(extracted_metadata['KapcsolodoHatarozatok'], (str, int, float, bool)):
                            extracted_metadata['KapcsolodoHatarozatok'] = json.dumps(extracted_metadata['KapcsolodoHatarozatok'], ensure_ascii=False)
            except json.JSONDecodeError:
                logging.warning(f"Nem siker√ºlt dek√≥dolni a JSON f√°jlt: {json_path}")
            except Exception as e:
                logging.warning(f"Hiba a JSON f√°jl feldolgoz√°sa k√∂zben ({json_path}): {e}")

        birosag_from_path = None
        try:
            abs_root_dir = root_dir_to_scan.resolve()
            abs_path = path.resolve()
            if abs_path.is_relative_to(abs_root_dir):
                 rel_parts = abs_path.relative_to(abs_root_dir).parts
                 if len(rel_parts) > 1:
                    birosag_from_path = rel_parts[0]
        except Exception as e_path:
             logging.warning(f"V√°ratlan hiba a b√≠r√≥s√°g nev√©nek √∫tvonalb√≥l t√∂rt√©n≈ë kinyer√©se k√∂zben ({path}): {e_path}")

        record = {
            'text': cleaned_text_content,
            **extracted_metadata,
            'AllKapcsolodoUgyszam': json.dumps(all_related_ugyszam, ensure_ascii=False) if all_related_ugyszam else None,
            'AllKapcsolodoBirosag': json.dumps(all_related_birosag, ensure_ascii=False) if all_related_birosag else None,
        }
        record['doc_id'] = extracted_metadata.get('Azonosito', base_filename)
        record['birosag'] = extracted_metadata.get('MeghozoBirosag', birosag_from_path)
        
        record.pop('Szoveg', None)
        record.pop('RezumeSzovegKornyezet', None)
        record.pop('DownloadLink', None)
        record.pop('metadata', None)

        all_records.append(record)
        total_records += 1

# ===== EGYES√çTETT, TISZT√çTOTT PARQUET L√âTREHOZ√ÅSA √âS MENT√âSE =====
logging.info("Feldolgoz√°s befejezve, egys√©ges DataFrame l√©trehoz√°sa...")

if all_records:
    try:
        df = pd.DataFrame(all_records)
        
        # A 'birosag' oszlop felt√∂lt√©se, ha hi√°nyzik (fontos a konzisztenci√°hoz)
        df['birosag'] = df['birosag'].fillna('ISMERETLEN')

        # Oszlopok sorrendj√©nek biztos√≠t√°sa a jobb √°tl√°that√≥s√°g√©rt
        expected_cols = [
            'doc_id', 'text', 'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',
            'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', 'AllKapcsolodoBirosag',
            'KapcsolodoHatarozatok', 'Jogszabalyhelyek'
        ]
        
        final_cols = [col for col in expected_cols if col in df.columns]
        other_cols = [col for col in df.columns if col not in final_cols]
        df = df[final_cols + other_cols]

        # A kimeneti √∫tvonal most a Parquet f√°jlra mutat
        out_path = config.CLEANED_PARQUET_DATA_PATH
        out_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Ment√©s egyetlen, t√∂m√∂r√≠tett Parquet f√°jlba
        df.to_parquet(
            path=out_path,
            engine='pyarrow',
            compression='snappy',
            index=False,
        )
        
        logging.info(f"Tiszt√≠tott Parquet f√°jl sikeresen mentve: {out_path} ({len(df):,} sor)")

    except Exception as e:
        logging.error(f"Hiba a Parquet f√°jl l√©trehoz√°s√°ban vagy ment√©s√©ben: {e}", exc_info=True)

# ===== V√âGS≈ê √úZENETEK =====
print(f"\n‚úÖ PREPROCESSING BEFEJEZVE!")
print(f"üìä Feldolgozott rekordok: {total_records:,}")
print(f"üìÑ Kimeneti f√°jl: {config.CLEANED_PARQUET_DATA_PATH}")