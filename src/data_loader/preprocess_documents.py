# Ez a szkript felel≈ës a nyers dokumentumok (RTF, DOCX) √©s a hozz√°juk tartoz√≥
# JSON metaadatok feldolgoz√°s√°√©rt, majd egyetlen CSV f√°jlba t√∂rt√©n≈ë ment√©s√©√©rt.
import pandas as pd
import json
import re
from pathlib import Path
from tqdm import tqdm
import sys
import os
import logging
from striprtf.striprtf import rtf_to_text
import csv
from docx import Document
from bs4 import BeautifulSoup
import html
import io

# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Konfigur√°ci√≥ √©s seg√©dprogramok import√°l√°sa
try:
    from configs import config
    from src.utils.azure_blob_storage import AzureBlobStorage
except ImportError as e:
    print(f"HIBA: Modul import√°l√°sa sikertelen: {e}")
    sys.exit(1)

# Loggol√°s be√°ll√≠t√°sa
logging.basicConfig(level=config.LOGGING_LEVEL, format=config.LOGGING_FORMAT)

# Azure Blob Storage kliens inicializ√°l√°sa
try:
    blob_storage = AzureBlobStorage(container_name=config.AZURE_CONTAINER_NAME)
except ValueError as e:
    logging.error(e)
    sys.exit(1)

def clean_text_for_embedding(text: str) -> str:
    """
    Sz√∂veg tiszt√≠t√°sa embedding gener√°l√°s el≈ëtt.
    Elt√°vol√≠tja a HTML tageket, URL-eket √©s egy√©b technikai zajt.
    """
    if not isinstance(text, str) or not text.strip():
        return ""
    
    # HTML entit√°sok dek√≥dol√°sa (pl. &amp; -> &)
    try:
        text = html.unescape(text)
    except Exception:
        pass # Ha hiba t√∂rt√©nik, a nyers sz√∂veggel megy√ºnk tov√°bb

    # HTML tagek elt√°vol√≠t√°sa BeautifulSoup-pal
    try:
        soup = BeautifulSoup(text, 'html.parser')
        text = soup.get_text(separator=' ')
    except Exception:
        # Ha a BeautifulSoup hib√°t dob, egyszer≈± regex-szel pr√≥b√°ljuk
        text = re.sub(r'<[^>]+>', '', text)
    
    # URL-ek, email c√≠mek elt√°vol√≠t√°sa
    text = re.sub(r'http\S+|www\S+|https\S+|\S+@\S+', '', text, flags=re.MULTILINE)

    # Null byte √©s egy√©b nem sz√∂veges vez√©rl≈ë karakterek elt√°vol√≠t√°sa
    text = text.replace('\x00', '')
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    # RTF specifikus maradv√°nyok elt√°vol√≠t√°sa, amik a konverzi√≥ ut√°n maradhatnak
    text = re.sub(r'\\[a-zA-Z]+\d*\s?', '', text)
    text = re.sub(r'[{}]', '', text)
    
    # T√∂bbsz√∂r√∂s sz√≥k√∂z√∂k, tabul√°torok, √∫j sorok cser√©je egyetlen sz√≥k√∂zre
    text = re.sub(r'\\s+', ' ', text).strip()
    
    return text

# T√°mogatott sz√∂vegf√°jl kiterjeszt√©sek
SUPPORTED_EXTENSIONS = tuple(ext.lower() for ext in config.SUPPORTED_TEXT_EXTENSIONS)

logging.info("Feldolgoz√°s kezd√©se Azure Blob Storage-b√≥l.")

# Blobs list√°z√°sa a 'raw' prefixszel
try:
    all_blob_paths = blob_storage.list_blobs(path_prefix=config.BLOB_RAW_DATA_DIR)
    logging.info(f"Tal√°lva {len(all_blob_paths):,} blob a '{config.BLOB_RAW_DATA_DIR}/' prefix alatt.")
except Exception as e:
    logging.error(f"Hiba a blobok list√°z√°sa k√∂zben: {e}")
    sys.exit(1)

# Sz√∂vegf√°jlok √©s a hozz√°juk tartoz√≥ JSON-ok √∂sszep√°ros√≠t√°sa
document_blobs = {}
json_blobs = {}

for blob_path in all_blob_paths:
    path = Path(blob_path)
    if path.suffix.lower() in SUPPORTED_EXTENSIONS:
        # 'raw/filename.rtf' -> 'filename'
        base_name = path.stem
        document_blobs[base_name] = blob_path
    elif path.suffix.lower() == '.json':
        # 'raw/filename.RTF_OBH.JSON' -> 'filename'
        base_name = path.stem.replace('.RTF_OBH', '')
        json_blobs[base_name] = blob_path

# A feldolgozott rekordokat egyetlen list√°ban gy≈±jtj√ºk
all_records = []
total_records = 0

logging.info(f"Feldolgoz√°sra v√°r√≥ dokumentum p√°rok sz√°ma: {len(document_blobs)}")

for base_filename, text_blob_path in tqdm(document_blobs.items(), desc="Dokumentum blobok feldolgoz√°sa"):
    text_path = Path(text_blob_path)
    json_blob_path = json_blobs.get(base_filename)

    text_content = ""
    try:
        text_data = blob_storage.download_data(text_blob_path)
        if text_path.suffix.lower() == '.rtf':
            try:
                # Az rtf_to_text stringet v√°r, ez√©rt dek√≥doljuk
                rtf_content = text_data.decode('utf-8', errors='ignore')
                text_content = rtf_to_text(rtf_content, errors="ignore")
            except Exception as e:
                logging.warning(f"Nem siker√ºlt kinyerni a sz√∂veget az RTF blobb√≥l ({text_blob_path}): {e}")
        elif text_path.suffix.lower() == '.docx':
            try:
                # A Document BytesIO-t v√°r
                doc = Document(io.BytesIO(text_data))
                text_content = ' \\n'.join(para.text for para in doc.paragraphs if para.text.strip())
            except Exception as e:
                logging.warning(f"Nem siker√ºlt kinyerni a sz√∂veget a DOCX blobb√≥l ({text_blob_path}): {e}")
    except Exception as e:
        logging.error(f"Hiba a blob let√∂lt√©se k√∂zben ({text_blob_path}): {e}")
        continue
    
    # A kinyert nyers sz√∂veg azonnali tiszt√≠t√°sa
    cleaned_text_content = clean_text_for_embedding(text_content)

    # Csak akkor dolgozzuk fel a rekordot, ha a tiszt√≠t√°s ut√°n is maradt √©rt√©kelhet≈ë sz√∂veg
    if len(cleaned_text_content) < config.CLEANING_MIN_TEXT_LENGTH:
        logging.debug(f"Dokumentum √°tugorva, mert a tiszt√≠tott sz√∂veg t√∫l r√∂vid: {text_path.name}")
        continue

    extracted_metadata = {}
    all_related_ugyszam = []
    all_related_birosag = []

    if json_blob_path:
        try:
            json_data = blob_storage.download_data(json_blob_path)
            # A json.loads bytes-ot vagy string-et v√°r, a dek√≥dol√°s biztons√°gosabb
            metadata_dict = json.loads(json_data.decode('utf-8', errors='ignore'))
            if 'List' in metadata_dict and isinstance(metadata_dict['List'], list) and len(metadata_dict['List']) > 0:
                extracted_metadata = metadata_dict['List'][0]
                if 'KapcsolodoHatarozatok' in extracted_metadata and isinstance(extracted_metadata['KapcsolodoHatarozatok'], list):
                    for related_case in extracted_metadata['KapcsolodoHatarozatok']:
                        if isinstance(related_case, dict):
                            all_related_ugyszam.append(related_case.get('KapcsolodoUgyszam'))
                            all_related_birosag.append(related_case.get('KapcsolodoBirosag'))
                        else:
                            logging.warning(f"A KapcsolodoHatarozatok lista egyik eleme nem sz√≥t√°r a {json_blob_path} blobban.")
                            all_related_ugyszam.append(None)
                            all_related_birosag.append(None)
                if 'Jogszabalyhelyek' in extracted_metadata and not isinstance(extracted_metadata['Jogszabalyhelyek'], (str, int, float, bool)):
                    extracted_metadata['Jogszabalyhelyek'] = json.dumps(extracted_metadata['Jogszabalyhelyek'], ensure_ascii=False)
                if 'KapcsolodoHatarozatok' in extracted_metadata and not isinstance(extracted_metadata['KapcsolodoHatarozatok'], (str, int, float, bool)):
                    extracted_metadata['KapcsolodoHatarozatok'] = json.dumps(extracted_metadata['KapcsolodoHatarozatok'], ensure_ascii=False)
        except json.JSONDecodeError:
            logging.warning(f"Nem siker√ºlt dek√≥dolni a JSON blobot: {json_blob_path}")
        except Exception as e:
            logging.warning(f"Hiba a JSON blob feldolgoz√°sa k√∂zben ({json_blob_path}): {e}")

    birosag_from_path = None
    try:
        # A 'raw/birosag_neve/aktaszam.rtf' form√°tumot felt√©telezz√ºk
        path_parts = text_path.parts
        if len(path_parts) > 2 and path_parts[0] == config.BLOB_RAW_DATA_DIR:
             birosag_from_path = path_parts[1]
    except Exception as e_path:
         logging.warning(f"V√°ratlan hiba a b√≠r√≥s√°g nev√©nek √∫tvonalb√≥l t√∂rt√©n≈ë kinyer√©se k√∂zben ({text_path}): {e_path}")

    record = {
        'text': cleaned_text_content,
        **extracted_metadata,
        'AllKapcsolodoUgyszam': json.dumps(all_related_ugyszam, ensure_ascii=False) if all_related_ugyszam else None,
        'AllKapcsolodoBirosag': json.dumps(all_related_birosag, ensure_ascii=False) if all_related_birosag else None,
    }
    record['doc_id'] = extracted_metadata.get('Azonosito', base_filename)
    record['birosag'] = extracted_metadata.get('MeghozoBirosag', birosag_from_path)
    
    record.pop('Szoveg', None)
    record.pop('RezumeSzovegKornyezet', None)
    record.pop('DownloadLink', None)
    record.pop('metadata', None)

    all_records.append(record)
    total_records += 1

# ===== EGYES√çTETT, TISZT√çTOTT PARQUET L√âTREHOZ√ÅSA √âS FELT√ñLT√âSE AZURE BLOB-BA =====
logging.info("Feldolgoz√°s befejezve, egys√©ges DataFrame l√©trehoz√°sa √©s felt√∂lt√©se...")

if all_records:
    try:
        df = pd.DataFrame(all_records)
        
        # A 'birosag' oszlop felt√∂lt√©se, ha hi√°nyzik (fontos a konzisztenci√°hoz)
        df['birosag'] = df['birosag'].fillna('ISMERETLEN')

        # Oszlopok sorrendj√©nek biztos√≠t√°sa a jobb √°tl√°that√≥s√°g√©rt
        expected_cols = [
            'doc_id', 'text', 'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',
            'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', 'AllKapcsolodoBirosag',
            'KapcsolodoHatarozatok', 'Jogszabalyhelyek'
        ]
        
        final_cols = [col for col in expected_cols if col in df.columns]
        other_cols = [col for col in df.columns if col not in final_cols]
        df = df[final_cols + other_cols]

        # DataFrame konvert√°l√°sa Parquet form√°tum√∫ bytes-objektumm√°
        parquet_buffer = io.BytesIO()
        df.to_parquet(
            path=parquet_buffer,
            engine='pyarrow',
            compression='snappy',
            index=False,
        )
        parquet_buffer.seek(0)

        # Felt√∂lt√©s az Azure Blob Storage-ba
        blob_path = config.BLOB_CLEANED_DOCUMENTS_PARQUET
        blob_storage.upload_data(data=parquet_buffer.getvalue(), blob_path=blob_path)
        
        logging.info(f"Tiszt√≠tott Parquet f√°jl sikeresen felt√∂ltve ide: {blob_path} ({len(df):,} sor)")

    except Exception as e:
        logging.error(f"Hiba a Parquet f√°jl l√©trehoz√°s√°ban vagy felt√∂lt√©s√©ben: {e}", exc_info=True)

# ===== V√âGS≈ê √úZENETEK =====
print(f"\n‚úÖ PREPROCESSING BEFEJEZVE!")
print(f"üìä Feldolgozott rekordok: {total_records:,}")
print(f"üìÑ Kimeneti blob: {config.AZURE_CONTAINER_NAME}/{config.BLOB_CLEANED_DOCUMENTS_PARQUET}")