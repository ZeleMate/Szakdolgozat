# src/embedding/create_embeddings_gemini_api.py

# ==============================================================================
# === 1. CSOMAGOK TELEP√çT√âSE ===
# ==============================================================================
# Futtassa ezt a cell√°t a sz√ºks√©ges csomagok telep√≠t√©s√©hez a Jupyter/Colab k√∂rnyezetben.
import sys
import subprocess

# A linter-bar√°t megold√°s a csomagok telep√≠t√©s√©re
packages = [
    "pandas", "numpy", "tqdm", "langchain", "pyarrow", 
    "python-dotenv", "google-generativeai"
]
try:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", *packages])
except subprocess.CalledProcessError as e:
    print(f"Hiba a csomagok telep√≠t√©se k√∂zben: {e}")

# ==============================================================================
# === 2. IMPORT√ÅL√ÅSOK √âS ALAPVET≈ê BE√ÅLL√çT√ÅSOK ===
# ==============================================================================
import pandas as pd
import numpy as np
import gc
import time
import os
import sys
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions
from tqdm.auto import tqdm
from pathlib import Path
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import pyarrow as pa
import pyarrow.parquet as pq

# ==============================================================================
# === 3. KONFIGUR√ÅCI√ìS V√ÅLTOZ√ìK ===
# ==============================================================================

# --- K√∂rnyezeti v√°ltoz√≥k bet√∂lt√©se ---
# FONTOS: Gy≈ëz≈ëdj√∂n meg r√≥la, hogy a GOOGLE_API_KEY v√°ltoz√≥ be van √°ll√≠tva.
load_dotenv()

# --- Helyi f√°jlbe√°ll√≠t√°sok ---
PROCESSED_DATA_DIR = Path("processed")
INPUT_PARQUET_PATH = PROCESSED_DATA_DIR / "cleaned_documents.parquet"
OUTPUT_PARQUET_PATH = PROCESSED_DATA_DIR / "documents_with_gemini_embeddings.parquet"

# --- Gemini API be√°ll√≠t√°sok ---
GEMINI_MODEL_NAME = "text-embedding-004"
GEMINI_TASK_TYPE = "RETRIEVAL_DOCUMENT"
GEMINI_BATCH_SIZE = 100
API_RETRY_ATTEMPTS = 5
API_RETRY_DELAY_SECONDS = 5

# --- Feldolgoz√°si be√°ll√≠t√°sok ---
WRITE_BATCH_SIZE = 50000 # Egyszerre ennyi sort dolgoz fel √©s √≠r ki

# --- Darabol√°s be√°ll√≠t√°sok ---
CHUNK_SIZE = 8000
CHUNK_OVERLAP = 200

# ==============================================================================
# === 4. FELDOLGOZ√ì OSZT√ÅLYOK ===
# ==============================================================================

class DocumentProcessor:
    """Felel≈ës egy dokumentum sz√∂veg√©nek darabol√°s√°√©rt (chunking)."""
    def __init__(self, chunk_size, chunk_overlap):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False
        )
    def split_document(self, doc_id, text):
        if not isinstance(text, str) or not text.strip():
            return []
        chunks_text = self.text_splitter.split_text(text)
        return [{'doc_id': doc_id, 'text_chunk': chunk} for chunk in chunks_text]

# ==============================================================================
# === 5. F≈ê VEZ√âRL≈ê LOGIKA ===
# ==============================================================================

def get_gemini_embeddings_with_retry(texts, model_name, task_type):
    """Gemini embeddingek lek√©r√©se √∫jrapr√≥b√°lkoz√°si logik√°val."""
    for attempt in range(API_RETRY_ATTEMPTS):
        try:
            result = genai.embed_content(
                model=model_name,
                content=texts,
                task_type=task_type
            )
            return result['embedding']
        except (google_exceptions.ResourceExhausted, google_exceptions.ServiceUnavailable) as e:
            print(f"API hiba (k√≠s√©rlet: {attempt + 1}/{API_RETRY_ATTEMPTS}): {e}. V√°rakoz√°s √©s √∫jrapr√≥b√°l√°s...")
            time.sleep(API_RETRY_DELAY_SECONDS * (2 ** attempt)) # Exponenci√°lis backoff
        except Exception as e:
            print(f"V√°ratlan hiba az API h√≠v√°s sor√°n: {e}")
            break
    return [np.nan] * len(texts)

def main():
    """F≈ë vez√©rl≈ë f√ºggv√©ny az embedding gener√°l√°shoz a Gemini API seg√≠ts√©g√©vel."""
    
    if not os.getenv("GOOGLE_API_KEY"):
        print("‚ùå HIBA: A GOOGLE_API_KEY k√∂rnyezeti v√°ltoz√≥ nincs be√°ll√≠tva.")
        return
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

    writer = None
    try:
        PROCESSED_DATA_DIR.mkdir(exist_ok=True)
        print(f"Adatok olvas√°sa innen: {INPUT_PARQUET_PATH}")
        if not INPUT_PARQUET_PATH.exists():
            print(f"‚ùå HIBA: A bemeneti f√°jl nem tal√°lhat√≥: {INPUT_PARQUET_PATH}")
            return
            
        main_start_time = time.time()
        df_full = pd.read_parquet(INPUT_PARQUET_PATH)
        
        if df_full.empty:
            print("‚ö†Ô∏è A bemeneti DataFrame √ºres.")
            return

        processor = DocumentProcessor(CHUNK_SIZE, CHUNK_OVERLAP)
        
        print("Sz√∂vegek darabol√°sa (chunking)...")
        all_chunk_records = []
        for _, row in tqdm(df_full.iterrows(), total=len(df_full), desc="Dokumentumok feldolgoz√°sa"):
            chunks = processor.split_document(row['doc_id'], row['text'])
            if chunks:
                meta_cols = {col: row.get(col) for col in df_full.columns if col != 'text'}
                for i, chunk_info in enumerate(chunks):
                    record = meta_cols.copy()
                    record['chunk_id'] = f"{chunk_info['doc_id']}-{i}"
                    record['text_chunk'] = chunk_info['text_chunk']
                    all_chunk_records.append(record)
        
        del df_full # Mem√≥ria felszabad√≠t√°sa
        gc.collect()

        if not all_chunk_records:
            print("‚ö†Ô∏è A dokumentumokb√≥l nem siker√ºlt darabokat k√©sz√≠teni.")
            return

        print(f"√ñsszesen {len(all_chunk_records):,} darab (chunk) k√©sz√ºlt.")
        
        print(f"\nEmbeddingek gener√°l√°sa √©s ment√©s {WRITE_BATCH_SIZE} soros darabokban...")

        for i in tqdm(range(0, len(all_chunk_records), WRITE_BATCH_SIZE), desc="F≈ë feldolgoz√°si ciklus"):
            batch_records = all_chunk_records[i:i + WRITE_BATCH_SIZE]
            
            if not batch_records:
                continue

            batch_df = pd.DataFrame(batch_records)

            text_chunks_list = batch_df['text_chunk'].tolist()
            all_embeddings = []
            for j in tqdm(range(0, len(text_chunks_list), GEMINI_BATCH_SIZE), desc="Gemini API h√≠v√°sok", leave=False):
                api_batch_texts = text_chunks_list[j:j + GEMINI_BATCH_SIZE]
                api_batch_embeddings = get_gemini_embeddings_with_retry(
                    texts=api_batch_texts,
                    model_name=GEMINI_MODEL_NAME,
                    task_type=GEMINI_TASK_TYPE
                )
                all_embeddings.extend(api_batch_embeddings)
            
            batch_df['embedding'] = all_embeddings
            
            failed_count = batch_df['embedding'].isna().sum()
            if failed_count > 0:
                print(f"‚ö†Ô∏è Darabon bel√ºl {failed_count} embedding gener√°l√°sa sikertelen volt.")
                batch_df.dropna(subset=['embedding'], inplace=True)

            if 'text' in batch_df.columns:
                batch_df = batch_df.drop(columns=['text'])

            # Adatok hozz√°f≈±z√©se a Parquet f√°jlhoz
            table = pa.Table.from_pandas(batch_df)
            if writer is None:
                writer = pq.ParquetWriter(OUTPUT_PARQUET_PATH, table.schema, compression='snappy')
            writer.write_table(table)
        
        print("\n‚úÖ Ment√©s sikeres.")

        total_time = time.time() - main_start_time
        print(f"\n--- Feldolgoz√°s befejezve {total_time:.2f} m√°sodperc alatt ---")

    finally:
        if writer:
            writer.close()
        print("üßπ Mem√≥ria felszabad√≠t√°sa...")
        gc.collect()

# ==============================================================================
# === 6. SCRIPT FUTTAT√ÅSA ===
# ==============================================================================
if __name__ == '__main__':
    main() 