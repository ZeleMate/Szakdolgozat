# src/embedding/create_embeddings_cloud.py

import pandas as pd
import numpy as np
import gc
import torch
import time
import os
import sys
import tempfile
import shutil
import io
from tqdm.auto import tqdm
from pathlib import Path
import torch.multiprocessing as mp
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- PATH KONFIGUR√ÅCI√ì ---
# Projekt gy√∂k√©rk√∂nyvt√°r√°nak hozz√°ad√°sa a Python √∫tvonalhoz
project_root = Path(__file__).resolve().parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

try:
    from configs import config
    from src.utils.azure_blob_storage import AzureBlobStorage
except ImportError as e:
    print(f"HIBA: Modul import√°l√°sa sikertelen: {e}")
    sys.exit(1)

# ==============================================================================
# === WORKER LOGIKA ===
# ==============================================================================

class DocumentProcessor:
    """Felel≈ës egy dokumentum sz√∂veg√©nek darabol√°s√°√©rt (chunking)."""
    def __init__(self, chunk_size, chunk_overlap):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False
        )
    def split_document(self, doc_id, text):
        if not isinstance(text, str) or not text.strip():
            return []
        chunks_text = self.text_splitter.split_text(text)
        return [{'doc_id': doc_id, 'text_chunk': chunk} for chunk in chunks_text]

class EmbeddingGenerator:
    """Felel≈ës az embedding modell bet√∂lt√©s√©√©rt √©s a sz√∂vegek embeddingj√©√©rt."""
    def __init__(self, model_name, batch_size, device):
        self.model = None
        self.model_name = model_name
        self.batch_size = batch_size
        self.device = device
    
    def load_model(self):
        if self.model is None:
            print(f"[{self.device}] Modell bet√∂lt√©se: {self.model_name}...")
            # A modell cache-el√©s√©t egy ideiglenes k√∂nyvt√°rba ir√°ny√≠tjuk, hogy ne szemelje tele a RunPod t√°rhelyet
            self.model = SentenceTransformer(
                self.model_name, 
                device=self.device, 
                trust_remote_code=True,
                cache_folder=os.path.join(tempfile.gettempdir(), 'sentence_transformers_cache')
            )
            print(f"[{self.device}] Modell bet√∂ltve.")
    
    def generate_embeddings(self, texts):
        with torch.autocast(device_type="cuda", dtype=torch.float16):
            return self.model.encode(
                texts,
                batch_size=self.batch_size,
                normalize_embeddings=True,
                show_progress_bar=False,
                convert_to_numpy=True
            ).astype(np.float32)

def process_data_chunk_on_worker(args):
    """
    Ez a f≈ë worker f√ºggv√©ny. Beolvas egy adatdarabot tartalmaz√≥ f√°jlt,
    legener√°lja az embeddingeket, √©s az eredm√©nyt egy kimeneti f√°jlba menti.
    """
    worker_id, device, model_name, batch_size, chunk_size, chunk_overlap, input_file_path, output_file_path = args
    
    try:
        print(f"[Worker {worker_id}]: Indul, feldolgozza: {input_file_path}")
        processor = DocumentProcessor(chunk_size, chunk_overlap)
        generator = EmbeddingGenerator(model_name, batch_size, device)
        generator.load_model()

        df_input = pd.read_parquet(input_file_path)
        
        all_chunks = []; original_docs_info = {}
        for _, row in df_input.iterrows():
            chunks = processor.split_document(row['doc_id'], row['text'])
            if chunks:
                all_chunks.extend(chunks)
                meta_cols = {col: row.get(col) for col in df_input.columns if col != 'text'}
                original_docs_info[row['doc_id']] = meta_cols

        if not all_chunks: return None

        df_chunks = pd.DataFrame(all_chunks)
        df_chunks['embedding'] = list(generator.generate_embeddings(df_chunks['text_chunk'].tolist()))
        agg_embeddings = df_chunks.groupby('doc_id')['embedding'].apply(lambda x: np.mean(np.vstack(x), axis=0))
        
        final_data = []
        for doc_id, doc_embedding in agg_embeddings.items():
            doc_info = original_docs_info.get(doc_id, {})
            doc_info['embedding'] = doc_embedding
            final_data.append(doc_info)
        
        result_df = pd.DataFrame(final_data)
        result_df.to_parquet(output_file_path)
        
        print(f"[Worker {worker_id}]: ‚úÖ Befejezte, eredm√©ny mentve: {output_file_path}")
        return output_file_path

    except Exception as e:
        import traceback
        print(f"\n\nCRITICAL ERROR IN WORKER {worker_id}: {e}")
        traceback.print_exc()
        return None
    finally:
        del processor, generator
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# ==============================================================================
# === F≈ê VEZ√âRL≈ê LOGIKA ===
# ==============================================================================

def main():
    """F≈ë vez√©rl≈ë f√ºggv√©ny az embedding gener√°l√°shoz."""
    if not torch.cuda.is_available():
        print("‚ùå HIBA: CUDA nem el√©rhet≈ë. A szkript csak GPU-s k√∂rnyezetben futtathat√≥.")
        return
        
    NUM_GPUS = torch.cuda.device_count()
    print(f"üî• Tal√°lt GPU-k sz√°ma: {NUM_GPUS}")

    # Azure Blob Storage kliens
    try:
        blob_storage = AzureBlobStorage(container_name=config.AZURE_CONTAINER_NAME)
    except ValueError as e:
        print(f"‚ùå Hiba az Azure kliens inicializ√°l√°sa k√∂zben: {e}")
        return

    # Ideiglenes k√∂nyvt√°r l√©trehoz√°sa a feladat idej√©re
    local_temp_dir = Path(tempfile.mkdtemp(prefix="embedding_job_"))
    print(f"üìÅ Ideiglenes k√∂nyvt√°r l√©trehozva: {local_temp_dir}")
    
    try:
        # 1. Bemeneti f√°jl let√∂lt√©se
        input_blob_path = config.BLOB_CLEANED_DOCUMENTS_PARQUET
        local_input_file = local_temp_dir / Path(input_blob_path).name

        print(f"‚¨áÔ∏è Bemeneti adatok let√∂lt√©se: {input_blob_path}")
        try:
            input_data = blob_storage.download_data(input_blob_path)
            with open(local_input_file, "wb") as f:
                f.write(input_data)
            del input_data
            gc.collect()
            print("‚úÖ Let√∂lt√©s sikeres.")
        except Exception as e:
            print(f"‚ùå Hiba a let√∂lt√©s sor√°n: {e}")
            return
            
        # 2. F≈ë feldolgoz√°si ciklus
        print("\n--- Feldolgoz√°s ind√≠t√°sa t√∂bb GPU-n ---")
        main_start_time = time.time()
        
        mp.set_start_method('spawn', force=True)
        
        df_full = pd.read_parquet(local_input_file)
        
        if df_full.empty:
            print("‚ö†Ô∏è A bemeneti DataFrame √ºres, nincs mit feldolgozni.")
            return

        df_chunks_for_gpus = np.array_split(df_full, NUM_GPUS)
        del df_full; gc.collect()

        worker_args = []
        
        print("Ideiglenes f√°jlok l√©trehoz√°sa a workereknek...")
        for i, df_worker_chunk in enumerate(df_chunks_for_gpus):
            if not df_worker_chunk.empty:
                input_path = local_temp_dir / f"input_worker_{i}.parquet"
                output_path = local_temp_dir / f"output_worker_{i}.parquet"
                
                df_worker_chunk.to_parquet(input_path)
                
                worker_args.append((
                    i, f'cuda:{i}', config.MODEL_NAME, config.BATCH_SIZE, 
                    config.CHUNK_SIZE, config.CHUNK_OVERLAP, 
                    str(input_path), str(output_path)
                ))

        # 3. P√°rhuzamos feldolgoz√°s
        print(f"Worker processzek ind√≠t√°sa ({len(worker_args)} db)...")
        with mp.Pool(processes=NUM_GPUS) as pool:
            result_paths = pool.map(process_data_chunk_on_worker, worker_args)
        
        # 4. Eredm√©nyek √∂sszef≈±z√©se
        print("\nAdatok √∂sszef≈±z√©se az ideiglenes f√°jlokb√≥l...")
        valid_result_paths = [p for p in result_paths if p is not None and Path(p).exists()]
        
        if valid_result_paths:
            final_df = pd.concat(
                [pd.read_parquet(f) for f in valid_result_paths], 
                ignore_index=True
            )
            
            # 5. V√©gs≈ë felt√∂lt√©s
            print(f"‚¨ÜÔ∏è Feldolgozott adatok felt√∂lt√©se ({len(final_df):,} sor)...")
            output_blob_path = config.BLOB_DOCUMENTS_WITH_EMBEDDINGS_PARQUET
            
            buffer = io.BytesIO()
            final_df.to_parquet(buffer, index=False, engine='pyarrow', compression='snappy')
            buffer.seek(0)
            
            blob_storage.upload_data(data=buffer.getvalue(), blob_path=output_blob_path)
            
            print(f"‚úÖ Felt√∂lt√©s sikeres ide: {config.AZURE_CONTAINER_NAME}/{output_blob_path}")
        else:
            print("‚ö†Ô∏è Nem keletkezett feldolgozott adat.")

        total_time = time.time() - main_start_time
        print(f"\n--- Feldolgoz√°s befejezve {total_time:.2f} m√°sodperc alatt ---")

    finally:
        # 6. Ideiglenes k√∂nyvt√°r t√∂rl√©se
        print(f"üóëÔ∏è Ideiglenes k√∂nyvt√°r t√∂rl√©se: {local_temp_dir}")
        shutil.rmtree(local_temp_dir, ignore_errors=True)

if __name__ == '__main__':
    main() 