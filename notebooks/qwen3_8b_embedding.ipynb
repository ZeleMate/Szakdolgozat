{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1. K√ñRNYEZET BE√ÅLL√çT√ÅSA ===\n",
        "# K√∂nyvt√°rak telep√≠t√©se √©s import√°l√°sa\n",
        "!pip install -U torch sentence-transformers accelerate pyarrow pandas tqdm transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# !!! KRITIKUS JAV√çT√ÅS: PyTorch mem√≥ria t√∂redezetts√©g√©nek kezel√©se !!!\n",
        "# A hiba√ºzenet javaslata alapj√°n be√°ll√≠tjuk ezt a k√∂rnyezeti v√°ltoz√≥t,\n",
        "# hogy a PyTorch rugalmasabban kezelje a GPU mem√≥ri√°t.\n",
        "# Ezt minden m√°s PyTorch m≈±velet el≈ëtt kell megtenni.\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# GPU optimaliz√°ci√≥ A100-hoz\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2. KONFIGUR√ÅCI√ì ===\n",
        "# RunPod felh≈ë k√∂rnyezethez igaz√≠tott konfigur√°ci√≥.\n",
        "\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "# --- CSV OLVAS√ÅSI LIMIT N√ñVEL√âSE ---\n",
        "try:\n",
        "    max_int = sys.maxsize\n",
        "    while True:\n",
        "        try:\n",
        "            csv.field_size_limit(max_int)\n",
        "            break\n",
        "        except OverflowError:\n",
        "            max_int = int(max_int / 10)\n",
        "except (ValueError, TypeError):\n",
        "    csv.field_size_limit(1_000_000_000)\n",
        "\n",
        "INPUT_CSV_PATH = Path(\"/workspace/cleaned_data_for_embedding.csv\")\n",
        "OUTPUT_PARQUET_PATH = Path(\"/workspace/documents_with_embeddings.parquet\")\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "EMBEDDING_DIMENSION = 1024\n",
        "# !!! V√âGS≈ê BIZTONS√ÅGI INT√âZKED√âS: BATCH M√âRET CS√ñKKENT√âSE !!!\n",
        "BATCH_SIZE = 128  # Tov√°bb cs√∂kkentj√ºk 256-r√≥l 128-ra\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "current_limit = csv.field_size_limit()\n",
        "logger.info(f\"CSV field size limit be√°ll√≠tva: {current_limit:,}\")\n",
        "\n",
        "logger.info(f\"Input: {INPUT_CSV_PATH}\")\n",
        "logger.info(f\"Output: {OUTPUT_PARQUET_PATH}\")\n",
        "logger.info(f\"Modell: {MODEL_NAME}\")\n",
        "logger.info(f\"Batch m√©ret: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3. EMBEDDING GENER√ÅTOR OSZT√ÅLY ===\n",
        "# Ez az oszt√°ly tiszta √©s √∂n√°ll√≥, csak az embedding gener√°l√°sra f√≥kusz√°l.\n",
        "class EmbeddingGenerator:\n",
        "    def __init__(self, model_name: str, batch_size: int, dimension: int, device: str = 'cuda'):\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "        self.dimension = dimension\n",
        "        self.device = device if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = None\n",
        "        logger.info(f\"Gener√°tor inicializ√°lva a(z) '{self.device}' eszk√∂z√∂n.\")\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.model is not None:\n",
        "            logger.info(\"Modell m√°r be van t√∂ltve.\")\n",
        "            return\n",
        "        try:\n",
        "            logger.info(f\"'{self.model_name}' modell bet√∂lt√©se...\")\n",
        "            self.model = SentenceTransformer(self.model_name, device=self.device, trust_remote_code=True)\n",
        "            self._warmup_model()\n",
        "            logger.info(\"Modell sikeresen bet√∂ltve √©s bemeleg√≠tve.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Modell bet√∂lt√©si hiba: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _warmup_model(self):\n",
        "        logger.info(\"Modell bemeleg√≠t√©se...\")\n",
        "        self.generate_embeddings([\"meleg√≠t√©s\"])\n",
        "        self._cleanup_memory()\n",
        "        logger.info(\"Bemeleg√≠t√©s k√©sz.\")\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"A modell nincs bet√∂ltve. H√≠vd meg a load_model() met√≥dust.\")\n",
        "        try:\n",
        "            embeddings = self.model.encode(\n",
        "                texts, \n",
        "                batch_size=self.batch_size, \n",
        "                normalize_embeddings=True, \n",
        "                show_progress_bar=True, # Legyen progress bar a konzolon\n",
        "                convert_to_numpy=True\n",
        "            )\n",
        "            if embeddings.shape[1] != self.dimension: # Biztons√°gi ellen≈ërz√©s\n",
        "                logger.warning(f\"V√°ratlan embedding dimenzi√≥: {embeddings.shape[1]}. Korrekci√≥ {self.dimension}-ra.\")\n",
        "                embeddings = embeddings[:, :self.dimension]\n",
        "            return embeddings.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            # R√©szletesebb logol√°s a hiba jobb meg√©rt√©s√©hez\n",
        "            problematic_text_snippet = texts[0][:200] if texts else \"√úres a sz√∂veg lista\"\n",
        "            logger.error(f\"!!! KRITIKUS HIBA AZ EMBEDDING GENER√ÅL√ÅSKOR !!!\")\n",
        "            logger.error(f\"Hiba√ºzenet: {e}\")\n",
        "            logger.error(f\"A hib√°t okoz√≥ batch els≈ë sz√∂veg√©nek r√©szlete (els≈ë 200 karakter): '{problematic_text_snippet}'\")\n",
        "            \n",
        "            # √öjra feldobjuk a hib√°t a teljes hiba-visszak√∂vet√©s√©rt (traceback)\n",
        "            raise\n",
        "\n",
        "    def _cleanup_memory(self):\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4. F≈ê FELDOLGOZ√ÅSI FOLYAMAT ===\n",
        "def create_metadata_json(row: pd.Series) -> str:\n",
        "    metadata_cols = [col for col in row.index if col not in ['text', 'embedding']]\n",
        "    metadata_dict = row[metadata_cols].dropna().to_dict()\n",
        "    return json.dumps({k: str(v) for k, v in metadata_dict.items()}, ensure_ascii=False)\n",
        "\n",
        "def main():\n",
        "    logger.info(\"Feldolgoz√°s ind√≠t√°sa...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Bemeneti adatok beolvas√°sa\n",
        "    if not INPUT_CSV_PATH.exists():\n",
        "        error_msg = f\"Hiba: A bemeneti f√°jl nem tal√°lhat√≥: {INPUT_CSV_PATH}\"\n",
        "        logger.error(error_msg)\n",
        "        raise FileNotFoundError(error_msg)\n",
        "    \n",
        "    logger.info(f\"Bemeneti CSV beolvas√°sa: {INPUT_CSV_PATH}\")\n",
        "    df = pd.read_csv(INPUT_CSV_PATH, engine='python', quoting=csv.QUOTE_ALL, on_bad_lines='warn')\n",
        "    logger.info(f\"{len(df):,} sor sikeresen beolvasva.\")\n",
        "\n",
        "    # Sz√∂vegek kinyer√©se √©s tiszt√≠t√°sa\n",
        "    df['text'] = df['text'].fillna('')\n",
        "    texts_to_process = df['text'].astype(str).tolist()\n",
        "    \n",
        "    if not texts_to_process:\n",
        "        logger.warning(\"Nincs feldolgozand√≥ sz√∂veg a bemeneti f√°jlban.\")\n",
        "        return\n",
        "\n",
        "    # Embedding gener√°tor inicializ√°l√°sa\n",
        "    generator = EmbeddingGenerator(MODEL_NAME, BATCH_SIZE, EMBEDDING_DIMENSION)\n",
        "    generator.load_model()\n",
        "\n",
        "    # --- MEM√ìRIAHAT√âKONY FELDOLGOZ√ÅS DARABOKBAN (CHUNK-OKBAN) ---\n",
        "    logger.info(\"Embedding gener√°l√°s megkezd√©se mem√≥riahat√©kony, darabolt m√≥dszerrel.\")\n",
        "    all_embeddings = []\n",
        "    # Biztons√°gi okokb√≥l cs√∂kkentett darabm√©ret\n",
        "    processing_chunk_size = 4096 \n",
        "\n",
        "    for i in tqdm(range(0, len(texts_to_process), processing_chunk_size), desc=\"Adatdarabok feldolgoz√°sa\"):\n",
        "        batch_texts = texts_to_process[i:i + processing_chunk_size]\n",
        "        batch_embeddings = generator.generate_embeddings(batch_texts)\n",
        "        all_embeddings.append(batch_embeddings)\n",
        "        \n",
        "        # !!! KRITIKUS JAV√çT√ÅS: GPU mem√≥ria felszabad√≠t√°sa minden darab ut√°n !!!\n",
        "        generator._cleanup_memory()\n",
        "            \n",
        "    # Az √∂sszes darab embeddingjeinek √∂sszef≈±z√©se\n",
        "    embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "    \n",
        "    # Eredm√©nyek hozz√°ad√°sa a DataFrame-hez\n",
        "    if len(embeddings) == len(df):\n",
        "        df['embedding'] = list(embeddings)\n",
        "    else:\n",
        "        logger.error(f\"KRITIKUS HIBA: Az embeddingek sz√°ma ({len(embeddings)}) nem egyezik a DataFrame sorainak sz√°m√°val ({len(df)}). A program le√°ll.\")\n",
        "        return\n",
        "\n",
        "    # Metaadatok gener√°l√°sa\n",
        "    tqdm.pandas(desc=\"Metaadat JSON gener√°l√°sa\")\n",
        "    df['metadata_json'] = df.progress_apply(create_metadata_json, axis=1)\n",
        "\n",
        "    # Kimeneti DataFrame √©s ment√©s Parquet form√°tumba\n",
        "    final_df = df[['doc_id', 'text', 'embedding', 'metadata_json']]\n",
        "    OUTPUT_PARQUET_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    logger.info(f\"Eredm√©nyek ment√©se a Parquet f√°jlba: {OUTPUT_PARQUET_PATH}\")\n",
        "    final_df.to_parquet(OUTPUT_PARQUET_PATH, index=False, compression='snappy')\n",
        "    \n",
        "    total_rows_processed = len(final_df)\n",
        "    total_time_seconds = time.time() - start_time\n",
        "    rows_per_second = total_rows_processed / total_time_seconds if total_time_seconds > 0 else 0\n",
        "    \n",
        "    # √ñsszegz√©s\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ FELDOLGOZ√ÅS BEFEJEZVE\")\n",
        "    print(f\"üìÑ Kimeneti f√°jl: {OUTPUT_PARQUET_PATH}\")\n",
        "    print(f\"‚è±Ô∏è Teljes id≈ë: {total_time_seconds:.2f} m√°sodperc ({total_time_seconds / 60:.2f} perc)\")\n",
        "    print(f\"üìä Feldolgozott sorok: {total_rows_processed:,}\")\n",
        "    print(f\"‚ö° √Åtlagos sebess√©g: {rows_per_second:.2f} sor/mp\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# F≈ë folyamat futtat√°sa\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5. VALID√ÅCI√ì ===\n",
        "logger.info(\"Kimeneti Parquet f√°jl valid√°l√°sa...\")\n",
        "\n",
        "if OUTPUT_PARQUET_PATH.exists():\n",
        "    try:\n",
        "        parquet_file = pq.ParquetFile(OUTPUT_PARQUET_PATH)\n",
        "        file_num_rows = parquet_file.metadata.num_rows\n",
        "        file_size_mb = OUTPUT_PARQUET_PATH.stat().st_size / (1024 * 1024)\n",
        "        \n",
        "        df_sample = pd.read_parquet(OUTPUT_PARQUET_PATH, engine='pyarrow', use_threads=True).head(5)\n",
        "        sample_embedding = df_sample['embedding'].iloc[0]\n",
        "        \n",
        "        print(\"\\n‚úÖ VALID√ÅCI√ì SIKERES!\")\n",
        "        print(f\"  F√°jl m√©ret: {file_size_mb:.2f} MB\")\n",
        "        print(f\"  Sorok sz√°ma: {file_num_rows:,}\")\n",
        "        print(f\"  Oszlopok: {df_sample.columns.tolist()}\")\n",
        "        print(f\"  Els≈ë embedding dimenzi√≥ja: {len(sample_embedding)}\")\n",
        "        print(\"\\n--- Minta Adatsor ---\")\n",
        "        display(df_sample)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Hiba a Parquet f√°jl valid√°l√°sa k√∂zben: {e}\")\n",
        "        print(f\"\\n‚ùå HIBA a valid√°ci√≥ sor√°n: {e}\")\n",
        "else:\n",
        "    logger.error(\"A kimeneti Parquet f√°jl nem j√∂tt l√©tre.\")\n",
        "    print(\"\\n‚ùå HIBA: A kimeneti f√°jl nem tal√°lhat√≥!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
