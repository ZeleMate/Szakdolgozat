{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 1. K√ñRNYEZET BE√ÅLL√çT√ÅSA ===\n",
        "# K√∂nyvt√°rak telep√≠t√©se √©s import√°l√°sa\n",
        "%pip install -U torch sentence-transformers accelerate pyarrow pandas tqdm transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import time\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# GPU optimaliz√°ci√≥ A100-hoz\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 2. KONFIGUR√ÅCI√ì ===\n",
        "# Felt√©telezz√ºk, hogy a notebook a 'notebooks' mapp√°ban van.\n",
        "# A projekt gy√∂kere a sz√ºl≈ë mappa.\n",
        "project_root = Path.cwd().parent\n",
        "\n",
        "# Bemeneti √∫tvonalak\n",
        "# Els≈ëdleges: egyetlen, nagy CSV f√°jl\n",
        "UNIFIED_CSV_PATH = project_root / \"processed_data\" / \"cleaned_data_for_embedding.csv\"\n",
        "# Fallback: a chunk-olt CSV-ket tartalmaz√≥ mappa\n",
        "CHUNKED_INPUT_DIR = project_root / \"processed_data\" / \"chunked_cleaned\"\n",
        "\n",
        "# Kimenet: Egyetlen Parquet f√°jl\n",
        "OUTPUT_PARQUET = project_root / \"processed_data\" / \"documents_with_embeddings.parquet\"\n",
        "\n",
        "# Modell √©s feldolgoz√°si param√©terek A100-ra optimaliz√°lva\n",
        "MODEL_NAME = \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "EMBEDDING_DIMENSION = 1024\n",
        "BATCH_SIZE = 256  # A100-on magasabb batch m√©retet haszn√°lhatunk\n",
        "UNIFIED_CSV_CHUNK_SIZE = 10000 # Mekkora darabokban olvassuk a nagy CSV-t\n",
        "\n",
        "# Logging be√°ll√≠t√°sa\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "logger.info(f\"Els≈ëdleges input: {UNIFIED_CSV_PATH}\")\n",
        "logger.info(f\"Fallback input: {CHUNKED_INPUT_DIR}\")\n",
        "logger.info(f\"Output f√°jl: {OUTPUT_PARQUET}\")\n",
        "logger.info(f\"Modell: {MODEL_NAME}\")\n",
        "logger.info(f\"Batch m√©ret: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 3. BEMENETI M√ìD MEGHAT√ÅROZ√ÅSA ===\n",
        "INPUT_MODE = None\n",
        "if UNIFIED_CSV_PATH.exists():\n",
        "    INPUT_MODE = 'UNIFIED'\n",
        "    logger.info(f\"‚úÖ Els≈ëdleges m√≥d kiv√°lasztva: Egyetlen CSV f√°jl feldolgoz√°sa ({UNIFIED_CSV_PATH}).\")\n",
        "elif CHUNKED_INPUT_DIR.exists() and any(CHUNKED_INPUT_DIR.glob('*.csv')):\n",
        "    INPUT_MODE = 'CHUNKED'\n",
        "    logger.info(f\"‚ö†Ô∏è  Fallback m√≥d kiv√°lasztva: Chunk-olt CSV-k feldolgoz√°sa ({CHUNKED_INPUT_DIR}).\")\n",
        "else:\n",
        "    error_msg = f\"Hiba: Sem az els≈ëdleges input ({UNIFIED_CSV_PATH}), sem a fallback input ({CHUNKED_INPUT_DIR}) nem tal√°lhat√≥.\"\n",
        "    logger.error(error_msg)\n",
        "    raise FileNotFoundError(error_msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 4. EMBEDDING GENER√ÅTOR OSZT√ÅLY ===\n",
        "# Ez az oszt√°ly tiszta √©s √∂n√°ll√≥, csak az embedding gener√°l√°sra f√≥kusz√°l.\n",
        "class EmbeddingGenerator:\n",
        "    def __init__(self, model_name: str, batch_size: int, dimension: int, device: str = 'cuda'):\n",
        "        self.model_name = model_name\n",
        "        self.batch_size = batch_size\n",
        "        self.dimension = dimension\n",
        "        self.device = device if torch.cuda.is_available() else 'cpu'\n",
        "        self.model = None\n",
        "        logger.info(f\"Gener√°tor inicializ√°lva a(z) '{self.device}' eszk√∂z√∂n.\")\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.model is not None:\n",
        "            logger.info(\"Modell m√°r be van t√∂ltve.\")\n",
        "            return\n",
        "        try:\n",
        "            logger.info(f\"'{self.model_name}' modell bet√∂lt√©se...\")\n",
        "            self.model = SentenceTransformer(self.model_name, device=self.device, trust_remote_code=True)\n",
        "            self._warmup_model()\n",
        "            logger.info(\"Modell sikeresen bet√∂ltve √©s bemeleg√≠tve.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Modell bet√∂lt√©si hiba: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _warmup_model(self):\n",
        "        logger.info(\"Modell bemeleg√≠t√©se...\")\n",
        "        self.generate_embeddings([\"meleg√≠t√©s\"])\n",
        "        self._cleanup_memory()\n",
        "        logger.info(\"Bemeleg√≠t√©s k√©sz.\")\n",
        "\n",
        "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"A modell nincs bet√∂ltve. H√≠vd meg a load_model() met√≥dust.\")\n",
        "        try:\n",
        "            embeddings = self.model.encode(texts, batch_size=self.batch_size, normalize_embeddings=True, show_progress_bar=False, convert_to_numpy=True)\n",
        "            if embeddings.shape[1] != self.dimension: # Biztons√°gi ellen≈ërz√©s\n",
        "                logger.warning(f\"V√°ratlan embedding dimenzi√≥: {embeddings.shape[1]}. Korrekci√≥ {self.dimension}-ra.\")\n",
        "                embeddings = embeddings[:, :self.dimension]\n",
        "            return embeddings.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Hiba az embedding gener√°l√°s k√∂zben: {e}\")\n",
        "            return np.full((len(texts), self.dimension), np.nan, dtype=np.float32)\n",
        "\n",
        "    def _cleanup_memory(self):\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 5. F≈ê FELDOLGOZ√ÅSI FOLYAMAT ===\n",
        "def create_metadata_json(row: pd.Series) -> str:\n",
        "    metadata_cols = [col for col in row.index if col not in ['text', 'embedding']]\n",
        "    metadata_dict = row[metadata_cols].dropna().to_dict()\n",
        "    return json.dumps({k: str(v) for k, v in metadata_dict.items()}, ensure_ascii=False)\n",
        "\n",
        "def process_and_write_chunk(df_chunk, generator, parquet_writer):\n",
        "    \"\"\"Feldolgoz egy dataframe chunk-ot √©s ki√≠rja a Parquet f√°jlba.\"\"\"\n",
        "    texts_to_process = df_chunk['text'].dropna().astype(str).tolist()\n",
        "    if not texts_to_process:\n",
        "        return 0\n",
        "\n",
        "    embeddings = generator.generate_embeddings(texts_to_process)\n",
        "    df_chunk['embedding'] = list(embeddings)\n",
        "    df_chunk['metadata_json'] = df_chunk.apply(create_metadata_json, axis=1)\n",
        "\n",
        "    final_df = df_chunk[['doc_id', 'text', 'embedding', 'metadata_json']]\n",
        "    pa_table = pa.Table.from_pandas(final_df, preserve_index=False)\n",
        "    \n",
        "    parquet_writer.write_table(pa_table)\n",
        "    return len(final_df)\n",
        "\n",
        "def main():\n",
        "    logger.info(f\"Feldolgoz√°s ind√≠t√°sa '{INPUT_MODE}' m√≥dban...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    generator = EmbeddingGenerator(MODEL_NAME, BATCH_SIZE, EMBEDDING_DIMENSION)\n",
        "    generator.load_model()\n",
        "    \n",
        "    total_rows_processed = 0\n",
        "    parquet_writer = None\n",
        "\n",
        "    try:\n",
        "        # A s√©ma meghat√°roz√°s√°hoz beolvasunk egyetlen sort\n",
        "        if INPUT_MODE == 'UNIFIED':\n",
        "            schema_df = pd.read_csv(UNIFIED_CSV_PATH, nrows=1, engine='python')\n",
        "        else: # CHUNKED\n",
        "            chunk_files_list = sorted(CHUNKED_INPUT_DIR.glob(\"cleaned_chunk_*.csv\"))\n",
        "            schema_df = pd.read_csv(chunk_files_list[0], nrows=1, engine='python')\n",
        "\n",
        "        schema_df['embedding'] = [np.zeros(EMBEDDING_DIMENSION, dtype=np.float32)]\n",
        "        schema_df['metadata_json'] = \"\"\n",
        "        output_schema = pa.Table.from_pandas(schema_df[['doc_id', 'text', 'embedding', 'metadata_json']], preserve_index=False).schema\n",
        "\n",
        "        OUTPUT_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
        "        parquet_writer = pq.ParquetWriter(OUTPUT_PARQUET, output_schema, compression='snappy')\n",
        "\n",
        "        # Feldolgoz√°s a kiv√°lasztott m√≥d szerint\n",
        "        if INPUT_MODE == 'UNIFIED':\n",
        "            # A nagy CSV darabonk√©nti olvas√°sa\n",
        "            row_count = sum(1 for row in open(UNIFIED_CSV_PATH)) -1\n",
        "            with tqdm(total=row_count, desc=\"Sorok feldolgoz√°sa\", unit=\"sor\") as pbar:\n",
        "                for df_chunk in pd.read_csv(UNIFIED_CSV_PATH, chunksize=UNIFIED_CSV_CHUNK_SIZE, engine='python'):\n",
        "                    rows_done = process_and_write_chunk(df_chunk, generator, parquet_writer)\n",
        "                    total_rows_processed += rows_done\n",
        "                    pbar.update(rows_done)\n",
        "        \n",
        "        elif INPUT_MODE == 'CHUNKED':\n",
        "            chunk_files = sorted(CHUNKED_INPUT_DIR.glob(\"cleaned_chunk_*.csv\"))\n",
        "            for chunk_file in tqdm(chunk_files, desc=\"Chunk f√°jlok feldolgoz√°sa\", unit=\"f√°jl\"):\n",
        "                df_chunk = pd.read_csv(chunk_file, engine='python')\n",
        "                total_rows_processed += process_and_write_chunk(df_chunk, generator, parquet_writer)\n",
        "\n",
        "    finally:\n",
        "        if parquet_writer:\n",
        "            parquet_writer.close()\n",
        "            logger.info(\"Parquet √≠r√≥ sikeresen lez√°rva.\")\n",
        "\n",
        "    # √ñsszegz√©s\n",
        "    total_time_seconds = time.time() - start_time\n",
        "    rows_per_second = total_rows_processed / total_time_seconds if total_time_seconds > 0 else 0\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ FELDOLGOZ√ÅS BEFEJEZVE\")\n",
        "    print(f\"üìÑ Kimeneti f√°jl: {OUTPUT_PARQUET}\")\n",
        "    print(f\"‚è±Ô∏è Teljes id≈ë: {total_time_seconds / 60:.2f} perc\")\n",
        "    print(f\"üìä Feldolgozott sorok: {total_rows_processed:,}\")\n",
        "    print(f\"‚ö° √Åtlagos sebess√©g: {rows_per_second:.2f} sor/mp\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# F≈ë folyamat futtat√°sa\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === 6. VALID√ÅCI√ì ===\n",
        "logger.info(\"Kimeneti Parquet f√°jl valid√°l√°sa...\")\n",
        "\n",
        "if OUTPUT_PARQUET.exists():\n",
        "    try:\n",
        "        parquet_file = pq.ParquetFile(OUTPUT_PARQUET)\n",
        "        file_num_rows = parquet_file.metadata.num_rows\n",
        "        file_size_mb = OUTPUT_PARQUET.stat().st_size / (1024 * 1024)\n",
        "        \n",
        "        df_sample = pd.read_parquet(OUTPUT_PARQUET, nrows=5)\n",
        "        sample_embedding = df_sample['embedding'].iloc[0]\n",
        "        \n",
        "        print(\"\\n‚úÖ VALID√ÅCI√ì SIKERES!\")\n",
        "        print(f\"  F√°jl m√©ret: {file_size_mb:.2f} MB\")\n",
        "        print(f\"  Sorok sz√°ma: {file_num_rows:,}\")\n",
        "        print(f\"  Oszlopok: {df_sample.columns.tolist()}\")\n",
        "        print(f\"  Els≈ë embedding dimenzi√≥ja: {len(sample_embedding)}\")\n",
        "        print(\"\\n--- Minta Adatsor ---\")\n",
        "        display(df_sample)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Hiba a Parquet f√°jl valid√°l√°sa k√∂zben: {e}\")\n",
        "        print(f\"\\n‚ùå HIBA a valid√°ci√≥ sor√°n: {e}\")\n",
        "else:\n",
        "    logger.error(\"A kimeneti Parquet f√°jl nem j√∂tt l√©tre.\")\n",
        "    print(\"\\n‚ùå HIBA: A kimeneti f√°jl nem tal√°lhat√≥!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
