{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RunPod A100 GPU - K√∂nyvt√°rak telep√≠t√©se √©s import√°l√°sa\n",
        "!pip install -U torch sentence-transformers accelerate pyarrow pandas tqdm transformers\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import psutil\n",
        "import time\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Any\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# A100 GPU optimaliz√°ci√≥\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "print(\"RunPod A100 k√∂rnyezet inicializ√°lva!\")\n",
        "print(f\"CUDA el√©rhet≈ë: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU mem√≥ria: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RunPod A100 konfigur√°ci√≥\n",
        "print(\"RunPod A100 konfigur√°ci√≥ be√°ll√≠t√°sa...\")\n",
        "\n",
        "# F√°jl el√©r√©si utak RunPod-on\n",
        "INPUT_CSV = \"/workspace/cleaned_data_for_embedding.csv\"\n",
        "OUTPUT_PARQUET = \"/workspace/processed_documents_with_embeddings.parquet\"\n",
        "\n",
        "# Qwen3-8B A100 optimaliz√°lt param√©terek\n",
        "MODEL_NAME = \"Qwen/Qwen3-Embedding-8B\"\n",
        "EMBEDDING_DIMENSION = 8192\n",
        "BATCH_SIZE = 32            # A100 optim√°lis\n",
        "CHUNK_SIZE = 5000          # Chunk m√©ret\n",
        "MAX_TOKEN_LENGTH = 8192\n",
        "USE_MIXED_PRECISION = True\n",
        "MEMORY_LIMIT_GB = 75       # A100 80GB-b√≥l 75GB haszn√°lata\n",
        "\n",
        "print(f\"Bemeneti CSV: {INPUT_CSV}\")\n",
        "print(f\"Kimeneti Parquet: {OUTPUT_PARQUET}\")\n",
        "print(f\"Modell: {MODEL_NAME}\")\n",
        "print(f\"Dimenzi√≥: {EMBEDDING_DIMENSION}\")\n",
        "print(f\"Batch m√©ret: {BATCH_SIZE}\")\n",
        "print(f\"Chunk m√©ret: {CHUNK_SIZE:,}\")\n",
        "print(f\"Mixed Precision: {USE_MIXED_PRECISION}\")\n",
        "print(f\"Mem√≥ria limit: {MEMORY_LIMIT_GB}GB\")\n",
        "\n",
        "# Logging konfigur√°ci√≥\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('/workspace/embedding_generation.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSV adatok bet√∂lt√©se √©s valid√°l√°sa\n",
        "logger.info(\"CSV adatok valid√°l√°sa...\")\n",
        "\n",
        "# F√°jl l√©tez√©s ellen≈ërz√©se\n",
        "if not os.path.exists(INPUT_CSV):\n",
        "    raise FileNotFoundError(f\"CSV f√°jl nem tal√°lhat√≥: {INPUT_CSV}\")\n",
        "\n",
        "# Mintaadatok bet√∂lt√©se strukt√∫ra ellen≈ërz√©shez\n",
        "df_sample = pd.read_csv(INPUT_CSV, nrows=1000)\n",
        "logger.info(f\"Minta bet√∂ltve: {len(df_sample)} sor\")\n",
        "\n",
        "# Teljes f√°jl m√©ret becsl√©se\n",
        "total_rows = sum(1 for _ in open(INPUT_CSV, 'r', encoding='utf-8')) - 1\n",
        "logger.info(f\"Becs√ºlt teljes sorok: {total_rows:,}\")\n",
        "\n",
        "# K√∂telez≈ë oszlopok ellen≈ërz√©se\n",
        "required_columns = ['text', 'doc_id']\n",
        "missing_columns = [col for col in required_columns if col not in df_sample.columns]\n",
        "if missing_columns:\n",
        "    raise ValueError(f\"Hi√°nyz√≥ k√∂telez≈ë oszlopok: {missing_columns}\")\n",
        "\n",
        "# Teljes metadata oszlop lista a preprocess_documents.py alapj√°n\n",
        "expected_metadata_columns = [\n",
        "    'doc_id', 'text', 'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',\n",
        "    'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', 'AllKapcsolodoBirosag',\n",
        "    'KapcsolodoHatarozatok', 'Jogszabalyhelyek'\n",
        "]\n",
        "\n",
        "# Jelenlegi oszlopok list√°z√°sa\n",
        "available_columns = list(df_sample.columns)\n",
        "metadata_columns_present = [col for col in expected_metadata_columns if col in available_columns]\n",
        "metadata_columns_missing = [col for col in expected_metadata_columns if col not in available_columns]\n",
        "\n",
        "print(\"CSV valid√°ci√≥ sikeres!\")\n",
        "print(f\"Teljes sorok: {total_rows:,}\")\n",
        "print(f\"√ñsszes oszlop: {len(available_columns)}\")\n",
        "print(f\"Jelenlev≈ë metadata oszlopok ({len(metadata_columns_present)}): {metadata_columns_present}\")\n",
        "if metadata_columns_missing:\n",
        "    print(f\"Hi√°nyz√≥ metadata oszlopok ({len(metadata_columns_missing)}): {metadata_columns_missing}\")\n",
        "\n",
        "# Sz√∂veg hossz statisztik√°k\n",
        "text_lengths = df_sample['text'].str.len()\n",
        "print(f\"\\nSz√∂veg hossz statisztik√°k (minta):\")\n",
        "print(f\"  √Åtlag: {text_lengths.mean():.0f} karakter\")\n",
        "print(f\"  Medi√°n: {text_lengths.median():.0f} karakter\")\n",
        "print(f\"  Min: {text_lengths.min():.0f} karakter\")\n",
        "print(f\"  Max: {text_lengths.max():.0f} karakter\")\n",
        "\n",
        "# Becs√ºlt feldolgoz√°si id≈ë\n",
        "estimated_batches = (total_rows + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "estimated_chunks = (total_rows + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "print(f\"\\nBecs√ºlt feldolgoz√°s:\")\n",
        "print(f\"  Chunk-ok sz√°ma: {estimated_chunks:,}\")\n",
        "print(f\"  Batch-ek sz√°ma: {estimated_batches:,}\")\n",
        "print(f\"  Becs√ºlt id≈ë: 2-3 √≥ra A100 GPU-n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qwen3-Embedding-8B modell oszt√°ly A100-ra optimaliz√°lva\n",
        "logger.info(\"Qwen3-Embedding-8B modell oszt√°ly l√©trehoz√°sa...\")\n",
        "\n",
        "class OptimizedQwen3EmbeddingGenerator:\n",
        "    def __init__(self):\n",
        "        self.model_name = MODEL_NAME\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.dimension = EMBEDDING_DIMENSION\n",
        "        self.max_tokens = MAX_TOKEN_LENGTH\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        \n",
        "        # Teljes√≠tm√©ny k√∂vet√©s\n",
        "        self.processed_count = 0\n",
        "        self.failed_count = 0\n",
        "        self.batch_times = []\n",
        "        self.peak_memory_usage = 0\n",
        "        \n",
        "        logger.info(f\"Device: {self.device}\")\n",
        "        \n",
        "        try:\n",
        "            # Modell bet√∂lt√©se\n",
        "            logger.info(\"Qwen3-8B modell bet√∂lt√©se...\")\n",
        "            self.model = SentenceTransformer(\n",
        "                self.model_name,\n",
        "                device=self.device,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            \n",
        "            # Mixed precision optimaliz√°ci√≥\n",
        "            if self.device == 'cuda' and USE_MIXED_PRECISION:\n",
        "                self.model.half()\n",
        "                logger.info(\"Mixed precision bekapcsolva\")\n",
        "            \n",
        "            # Modell warmup\n",
        "            self._warmup_model()\n",
        "            logger.info(\"Modell sikeresen bet√∂ltve √©s optimaliz√°lva\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Modell bet√∂lt√©si hiba: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _warmup_model(self):\n",
        "        \"\"\"Modell warmup konzisztens teljes√≠tm√©ny√©rt\"\"\"\n",
        "        logger.info(\"Modell warmup...\")\n",
        "        dummy_texts = [\"Ez egy teszt sz√∂veg a modell bemeleg√≠t√©s√©hez.\"] * 4\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled=USE_MIXED_PRECISION):\n",
        "            _ = self.model.encode(dummy_texts, show_progress_bar=False)\n",
        "        \n",
        "        self._cleanup_memory()\n",
        "        logger.info(\"Warmup befejezve\")\n",
        "    \n",
        "    def _cleanup_memory(self):\n",
        "        \"\"\"Mem√≥ria tiszt√≠t√°s\"\"\"\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.synchronize()\n",
        "    \n",
        "    def _monitor_memory(self):\n",
        "        \"\"\"GPU mem√≥ria monitoring\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return {}\n",
        "        \n",
        "        allocated = torch.cuda.memory_allocated() / (1024**3)\n",
        "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
        "        \n",
        "        self.peak_memory_usage = max(self.peak_memory_usage, allocated)\n",
        "        \n",
        "        return {\n",
        "            'allocated_gb': allocated,\n",
        "            'reserved_gb': reserved,\n",
        "            'peak_usage_gb': self.peak_memory_usage\n",
        "        }\n",
        "\n",
        "# Embedding gener√°tor inicializ√°l√°sa\n",
        "embedding_generator = OptimizedQwen3EmbeddingGenerator()\n",
        "print(\"Qwen3-8B modell sikeresen inicializ√°lva!\")\n",
        "print(f\"Dimenzi√≥: {embedding_generator.dimension}\")\n",
        "print(f\"Device: {embedding_generator.device}\")\n",
        "print(f\"Mixed Precision: {USE_MIXED_PRECISION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding gener√°l√°s met√≥dus hozz√°ad√°sa\n",
        "def generate_embeddings_batch(self, texts):\n",
        "    \"\"\"A100 optimaliz√°lt batch embedding gener√°l√°s\"\"\"\n",
        "    batch_start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Sz√∂veg el≈ëfeldolgoz√°s\n",
        "        processed_texts = []\n",
        "        for text in texts:\n",
        "            if len(text) > self.max_tokens * 3:  # ~3 char/token becsl√©s\n",
        "                text = text[:self.max_tokens * 3]\n",
        "            processed_texts.append(text)\n",
        "        \n",
        "        # Mixed precision embedding gener√°l√°s\n",
        "        with torch.cuda.amp.autocast(enabled=USE_MIXED_PRECISION):\n",
        "            embeddings = self.model.encode(\n",
        "                processed_texts,\n",
        "                normalize_embeddings=True,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True,\n",
        "                batch_size=len(processed_texts)\n",
        "            )\n",
        "        \n",
        "        # Dimenzi√≥ ellen≈ërz√©s √©s korrekci√≥\n",
        "        if embeddings.shape[1] != self.dimension:\n",
        "            if embeddings.shape[1] > self.dimension:\n",
        "                embeddings = embeddings[:, :self.dimension]\n",
        "            else:\n",
        "                padding = np.zeros((embeddings.shape[0], self.dimension - embeddings.shape[1]))\n",
        "                embeddings = np.hstack([embeddings, padding])\n",
        "        \n",
        "        # Teljes√≠tm√©ny k√∂vet√©s\n",
        "        batch_time = time.time() - batch_start_time\n",
        "        self.batch_times.append(batch_time)\n",
        "        self.processed_count += len(texts)\n",
        "        \n",
        "        return embeddings.astype(np.float32)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Batch feldolgoz√°si hiba: {e}\")\n",
        "        self.failed_count += len(texts)\n",
        "        # Fallback: NaN vektorok\n",
        "        return np.full((len(texts), self.dimension), np.nan, dtype=np.float32)\n",
        "    \n",
        "    finally:\n",
        "        # Rendszeres mem√≥ria cleanup\n",
        "        if self.processed_count % 1000 == 0:\n",
        "            self._cleanup_memory()\n",
        "\n",
        "# Met√≥dus hozz√°ad√°sa az oszt√°lyhoz\n",
        "OptimizedQwen3EmbeddingGenerator.generate_embeddings_batch = generate_embeddings_batch\n",
        "\n",
        "print(\"Embedding gener√°l√°s met√≥dus hozz√°adva!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seg√©df√ºggv√©nyek\n",
        "def create_metadata_json(row):\n",
        "    \"\"\"Teljes metadata JSON k√©sz√≠t√©se az √∂sszes el√©rhet≈ë oszloppal\"\"\"\n",
        "    metadata = {\n",
        "        'doc_id': str(row.get('doc_id', '')),\n",
        "        'birosag': str(row.get('birosag', '')),\n",
        "        'JogTerulet': str(row.get('JogTerulet', '')),\n",
        "        'Azonosito': str(row.get('Azonosito', '')),\n",
        "        'MeghozoBirosag': str(row.get('MeghozoBirosag', '')),\n",
        "        'EgyediAzonosito': str(row.get('EgyediAzonosito', '')),\n",
        "        'HatarozatEve': str(row.get('HatarozatEve', '')),\n",
        "        'AllKapcsolodoUgyszam': str(row.get('AllKapcsolodoUgyszam', '')),\n",
        "        'AllKapcsolodoBirosag': str(row.get('AllKapcsolodoBirosag', '')),\n",
        "        'KapcsolodoHatarozatok': str(row.get('KapcsolodoHatarozatok', '')),\n",
        "        'Jogszabalyhelyek': str(row.get('Jogszabalyhelyek', '')),\n",
        "        'text_length': len(str(row.get('text', ''))),\n",
        "        'processed_timestamp': time.time()\n",
        "    }\n",
        "    return json.dumps(metadata, ensure_ascii=False)\n",
        "\n",
        "def adaptive_batch_size(text_lengths, base_batch_size=BATCH_SIZE):\n",
        "    \"\"\"Adapt√≠v batch m√©ret sz√∂veg hossz alapj√°n\"\"\"\n",
        "    avg_length = np.mean(text_lengths)\n",
        "    \n",
        "    if avg_length > 6000:\n",
        "        return max(8, base_batch_size // 4)\n",
        "    elif avg_length > 4000:\n",
        "        return max(16, base_batch_size // 2)\n",
        "    elif avg_length > 2000:\n",
        "        return base_batch_size\n",
        "    else:\n",
        "        return min(64, base_batch_size * 2)\n",
        "\n",
        "def prepare_final_columns(chunk_df):\n",
        "    \"\"\"V√©gs≈ë oszlopok el≈ëk√©sz√≠t√©se - √∂sszes metadata meg≈ërz√©se\"\"\"\n",
        "    # Alapvet≈ë oszlopok (k√∂telez≈ë)\n",
        "    final_columns = ['doc_id', 'text', 'embedding', 'metadata_json']\n",
        "    \n",
        "    # √ñsszes metadata oszlop hozz√°ad√°sa, ha l√©tezik\n",
        "    metadata_columns = [\n",
        "        'birosag', 'JogTerulet', 'Azonosito', 'MeghozoBirosag',\n",
        "        'EgyediAzonosito', 'HatarozatEve', 'AllKapcsolodoUgyszam', \n",
        "        'AllKapcsolodoBirosag', 'KapcsolodoHatarozatok', 'Jogszabalyhelyek'\n",
        "    ]\n",
        "    \n",
        "    # Csak a l√©tez≈ë oszlopokat adjuk hozz√°\n",
        "    for col in metadata_columns:\n",
        "        if col in chunk_df.columns:\n",
        "            final_columns.append(col)\n",
        "    \n",
        "    # Visszaadjuk a l√©tez≈ë oszlopokat\n",
        "    available_columns = [col for col in final_columns if col in chunk_df.columns]\n",
        "    return available_columns\n",
        "\n",
        "print(\"Seg√©df√ºggv√©nyek bet√∂ltve!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A100 f≈ëfolyamat - Robosztus embedding gener√°l√°s\n",
        "def process_embeddings_a100():\n",
        "    \"\"\"\n",
        "    A100 GPU-ra optimaliz√°lt robosztus embedding gener√°l√°s\n",
        "    \"\"\"\n",
        "    \n",
        "    start_time = time.time()\n",
        "    logger.info(\"A100 embedding feldolgoz√°s kezd√©se...\")\n",
        "    \n",
        "    total_rows = sum(1 for _ in open(INPUT_CSV, 'r', encoding='utf-8')) - 1\n",
        "    processed_rows = 0\n",
        "    all_results = []\n",
        "    \n",
        "    logger.info(f\"Feldolgozand√≥ sorok: {total_rows:,}\")\n",
        "    logger.info(f\"Chunk m√©ret: {CHUNK_SIZE:,}\")\n",
        "    logger.info(f\"Batch m√©ret: {BATCH_SIZE}\")\n",
        "    \n",
        "    # Chunk-alap√∫ feldolgoz√°s\n",
        "    chunk_count = 0\n",
        "    total_chunks = (total_rows + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
        "    \n",
        "    with tqdm(total=total_chunks, desc=\"Chunk feldolgoz√°s\", unit=\"chunk\") as chunk_pbar:\n",
        "        \n",
        "        for chunk_df in pd.read_csv(INPUT_CSV, chunksize=CHUNK_SIZE, encoding='utf-8'):\n",
        "            chunk_count += 1\n",
        "            chunk_start_time = time.time()\n",
        "            \n",
        "            # Adatok tiszt√≠t√°sa\n",
        "            original_len = len(chunk_df)\n",
        "            chunk_df = chunk_df.dropna(subset=['text', 'doc_id'])\n",
        "            chunk_df['text'] = chunk_df['text'].astype(str)\n",
        "            chunk_df = chunk_df[chunk_df['text'].str.len() > 10]  # Min sz√∂veghossz\n",
        "            \n",
        "            if len(chunk_df) == 0:\n",
        "                logger.warning(f\"Chunk {chunk_count}: nincs √©rv√©nyes adat\")\n",
        "                chunk_pbar.update(1)\n",
        "                continue\n",
        "            \n",
        "            logger.info(f\"Chunk {chunk_count}/{total_chunks}: {len(chunk_df):,} √©rv√©nyes sor\")\n",
        "            \n",
        "            # Sz√∂vegek √©s adapt√≠v batch m√©ret\n",
        "            texts = chunk_df['text'].tolist()\n",
        "            text_lengths = [len(text) for text in texts]\n",
        "            dynamic_batch_size = adaptive_batch_size(text_lengths, BATCH_SIZE)\n",
        "            \n",
        "            # Batch-es embedding gener√°l√°s\n",
        "            all_embeddings = []\n",
        "            total_batches_in_chunk = (len(texts) + dynamic_batch_size - 1) // dynamic_batch_size\n",
        "            \n",
        "            with tqdm(total=total_batches_in_chunk, desc=f\"Chunk {chunk_count} batch-ek\", \n",
        "                     unit=\"batch\", leave=False) as batch_pbar:\n",
        "                \n",
        "                for batch_idx in range(0, len(texts), dynamic_batch_size):\n",
        "                    batch_texts = texts[batch_idx:batch_idx + dynamic_batch_size]\n",
        "                    \n",
        "                    # Embedding gener√°l√°s hibakezel√©ssel\n",
        "                    try:\n",
        "                        batch_embeddings = embedding_generator.generate_embeddings_batch(batch_texts)\n",
        "                        all_embeddings.extend(batch_embeddings.tolist())\n",
        "                        \n",
        "                        # Mem√≥ria monitoring\n",
        "                        memory_info = embedding_generator._monitor_memory()\n",
        "                        if memory_info.get('allocated_gb', 0) > MEMORY_LIMIT_GB * 0.9:\n",
        "                            logger.warning(f\"Magas mem√≥ria: {memory_info.get('allocated_gb', 0):.1f}GB\")\n",
        "                            embedding_generator._cleanup_memory()\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        logger.error(f\"Batch hiba: {e}\")\n",
        "                        # Fallback NaN vektorok\n",
        "                        nan_embeddings = np.full((len(batch_texts), EMBEDDING_DIMENSION), np.nan)\n",
        "                        all_embeddings.extend(nan_embeddings.tolist())\n",
        "                    \n",
        "                    batch_pbar.update(1)\n",
        "            \n",
        "            # Embedding sz√°moss√°gi ellen≈ërz√©s\n",
        "            if len(all_embeddings) != len(chunk_df):\n",
        "                logger.error(f\"Embedding sz√°moss√°gi hiba: {len(all_embeddings)} != {len(chunk_df)}\")\n",
        "                # Kieg√©sz√≠t√©s NaN-okkal\n",
        "                while len(all_embeddings) < len(chunk_df):\n",
        "                    all_embeddings.append(np.full(EMBEDDING_DIMENSION, np.nan).tolist())\n",
        "            \n",
        "            # Eredm√©nyek hozz√°ad√°sa\n",
        "            chunk_df['embedding'] = all_embeddings\n",
        "            chunk_df['metadata_json'] = chunk_df.apply(create_metadata_json, axis=1)\n",
        "            \n",
        "            # V√©gs≈ë oszlopok - √∂sszes metadata meg≈ërz√©se\n",
        "            available_columns = prepare_final_columns(chunk_df)\n",
        "            chunk_result = chunk_df[available_columns].copy()\n",
        "            \n",
        "            all_results.append(chunk_result)\n",
        "            processed_rows += len(chunk_df)\n",
        "            \n",
        "            # Progress update\n",
        "            chunk_time = time.time() - chunk_start_time\n",
        "            rows_per_sec = len(chunk_df) / chunk_time\n",
        "            \n",
        "            chunk_pbar.set_postfix({\n",
        "                'Sorok/sec': f'{rows_per_sec:.1f}',\n",
        "                'Mem√≥ria': f'{embedding_generator._monitor_memory().get(\"allocated_gb\", 0):.1f}GB',\n",
        "                'Sikeres': embedding_generator.processed_count,\n",
        "                'Hib√°s': embedding_generator.failed_count\n",
        "            })\n",
        "            chunk_pbar.update(1)\n",
        "            \n",
        "            # Rendszeres cleanup\n",
        "            if chunk_count % 3 == 0:\n",
        "                embedding_generator._cleanup_memory()\n",
        "    \n",
        "    # Eredm√©nyek egyes√≠t√©se\n",
        "    logger.info(\"DataFrame-ek egyes√≠t√©se...\")\n",
        "    if not all_results:\n",
        "        raise ValueError(\"Nincs feldolgozott adat!\")\n",
        "    \n",
        "    final_df = pd.concat(all_results, ignore_index=True)\n",
        "    logger.info(f\"Egyes√≠tett DataFrame: {len(final_df):,} sor\")\n",
        "    \n",
        "    return final_df, processed_rows, time.time() - start_time\n",
        "\n",
        "# A100 f≈ëfolyamat ind√≠t√°sa\n",
        "logger.info(\"A100 embedding feldolgoz√°s ind√≠t√°sa...\")\n",
        "final_df, processed_rows, total_time = process_embeddings_a100()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parquet ment√©s √©s v√©gs≈ë valid√°ci√≥\n",
        "logger.info(\"Parquet ment√©s √©s valid√°ci√≥...\")\n",
        "\n",
        "# Embedding valid√°ci√≥\n",
        "valid_embeddings = 0\n",
        "nan_embeddings = 0\n",
        "dimension_errors = 0\n",
        "\n",
        "for idx, emb in enumerate(final_df['embedding']):\n",
        "    if isinstance(emb, list):\n",
        "        if len(emb) == EMBEDDING_DIMENSION:\n",
        "            if not np.any(np.isnan(emb)):\n",
        "                valid_embeddings += 1\n",
        "            else:\n",
        "                nan_embeddings += 1\n",
        "        else:\n",
        "            dimension_errors += 1\n",
        "    else:\n",
        "        dimension_errors += 1\n",
        "\n",
        "logger.info(f\"Embedding valid√°ci√≥:\")\n",
        "logger.info(f\"  √ârv√©nyes: {valid_embeddings:,}\")\n",
        "logger.info(f\"  NaN: {nan_embeddings:,}\")\n",
        "logger.info(f\"  Dimenzi√≥ hiba: {dimension_errors:,}\")\n",
        "\n",
        "# Parquet ment√©s\n",
        "logger.info(f\"V√©gs≈ë Parquet ment√©s: {OUTPUT_PARQUET}\")\n",
        "\n",
        "final_df.to_parquet(\n",
        "    OUTPUT_PARQUET,\n",
        "    engine='pyarrow',\n",
        "    index=False,\n",
        "    compression='snappy',\n",
        "    row_group_size=50000\n",
        ")\n",
        "\n",
        "# F√°jl valid√°ci√≥\n",
        "file_size = os.path.getsize(OUTPUT_PARQUET) / (1024**3)\n",
        "\n",
        "# Gyors visszaolvas√°si teszt\n",
        "test_df = pd.read_parquet(OUTPUT_PARQUET, nrows=100)\n",
        "logger.info(f\"Visszaolvas√°si teszt sikeres: {len(test_df)} sor\")\n",
        "\n",
        "# V√©gs≈ë statisztik√°k\n",
        "logger.info(\"A100 QWEN3-8B EMBEDDING GENER√ÅL√ÅS BEFEJEZVE!\")\n",
        "logger.info(f\"Feldolgozott sorok: {processed_rows:,}\")\n",
        "logger.info(f\"V√©gs≈ë sorok: {len(final_df):,}\")\n",
        "logger.info(f\"V√©gs≈ë oszlopok ({len(final_df.columns)}): {list(final_df.columns)}\")\n",
        "logger.info(f\"√ârv√©nyes embeddings: {valid_embeddings:,}\")\n",
        "logger.info(f\"F√°jl m√©ret: {file_size:.2f}GB\")\n",
        "logger.info(f\"Teljes fut√°si id≈ë: {total_time/3600:.2f} √≥ra\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"QWEN3-8B EMBEDDING FELDOLGOZ√ÅS BEFEJEZVE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"üìä Feldolgozott dokumentumok: {processed_rows:,}\")\n",
        "print(f\"üìÅ V√©gs≈ë Parquet f√°jl: {OUTPUT_PARQUET}\")\n",
        "print(f\"üìà Oszlopok sz√°ma: {len(final_df.columns)}\")\n",
        "print(f\"üéØ √ârv√©nyes embeddings: {valid_embeddings:,}\")\n",
        "print(f\"üíæ F√°jl m√©ret: {file_size:.2f}GB\")\n",
        "print(f\"‚è±Ô∏è  Fut√°si id≈ë: {total_time/3600:.2f} √≥ra\")\n",
        "print(\"=\"*80)\n",
        "logger.info(f\"Teljes fut√°si id≈ë: {total_time/3600:.2f} √≥ra\")\n",
        "logger.info(f\"√Åtlag sebess√©g: {processed_rows/total_time:.1f} sor/sec\")\n",
        "logger.info(f\"F√°jl m√©ret: {file_size:.2f} GB\")\n",
        "logger.info(f\"Cs√∫cs mem√≥ria: {embedding_generator.peak_memory_usage:.1f}GB\")\n",
        "\n",
        "print(\"\\nA100 QWEN3-8B EMBEDDING GENER√ÅL√ÅS SIKERESEN BEFEJEZVE!\")\n",
        "print(f\"Feldolgozott sorok: {processed_rows:,}\")\n",
        "print(f\"√ârv√©nyes embeddings: {valid_embeddings:,}\")\n",
        "print(f\"F√°jl m√©ret: {file_size:.2f} GB\")\n",
        "print(f\"Teljes id≈ë: {total_time/3600:.2f} √≥ra\")\n",
        "print(f\"Sebess√©g: {processed_rows/total_time:.1f} sor/sec\")\n",
        "print(f\"Cs√∫cs mem√≥ria: {embedding_generator.peak_memory_usage:.1f}GB\")\n",
        "print(f\"Sikeress√©gi ar√°ny: {(valid_embeddings/len(final_df)*100):.1f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
